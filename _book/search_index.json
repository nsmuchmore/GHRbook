[
["index.html", "Global Health Research: Designs and Methods Preface About this Book Organization Icons Acknowledgements Colophon", " Global Health Research: Designs and Methods Eric P. Green 2017-03-27 Preface Does the world really need another book about research methods? I think so. But I spent a fair amount of time writing down the ideas in this book, so I’m biased. Here’s my rationale. I went to graduate school for clinical psychology, and my classmates and I read the classic psychology texts on research design and methods—books like Experimental and Quasi-Experimental Designs for Generalized Causal Inference by Shadish, Cook, and Campbell (2003). I remember staying up late trying to memorize all of the different threats to internal validity outlined by Donald Campbell and colleagues. Meanwhile, across campus, my econ colleagues were reading the ideas of another Donald—Donald Rubin—and what is now known as Rubin’s causal model. When I set off for Uganda in 2007, determined to learn more about this field called global health, I met some Donald Rubin proselytes in the wild. I tried communicating with them, but they did not understand my Campbellian drawl. We were usually trying to say the same thing, just in different the languages. But I couldn’t place the blame on the economists and the disciplinary gap between us. So much of what I didn’t know went far beyond differences in jargon. I was a psychologist trained in clinical research, and nearly every applied example I read came from the U.S. or Europe. The field of global mental health was still in its infancy when I was in school. The important Lancet series on global mental health that really put the field on the map was published in September 2007 as I was getting on a plane to fly back home. I really knew nothing about global health. Fortunately, students entering university today have many more opportunities to learn about global health through interdisciplinary studies. Duke University launched the first liberal arts global health major in the U.S. in 2013, and other universities have followed suit. The Duke program is unique because it requires global health students to specialize in a second discipline, such as biology, economics, psychology, or public policy. I started teaching at Duke around the time the new major started, and I needed to choose a textbook for a course called “Research Methods in Global Health.” After reviewing many excellent books that covered the basics, I discovered none that integrated examples from this very diverse and interdisciplinary field. So I decided to write my own book. About this Book One guiding principle of this book is that a student of global health needs to be a student of medicine, biology, statistics, economics, psychology, public policy, and many other disciplines. In the study of malaria, for example, a literature search returns articles about the spread of the disease (epidemiology), the impact of illness on future productivity (economics), the merits of free or subsidized bed nets (public policy), mosquito habitats (ecology), the efficacy of vaccines to prevent the disease (medicine and statistics), rapid diagnostic tests (biomedical engineering), the adoption and use of bed nets (psychology), and many others. No one book or author could ever hope to provide full disciplinary coverage of even a single topic like malaria. Therefore, I wanted to create a resource that would teach the basics of research design and methods by exposing readers to real world global health examples from different disciplines. Another important guiding principle of this book is openness. Whenever possible, the examples come from open access sources. In this way, every reader should be able to access at least 90% of the references provided here. Organization The first objective of my course on global health research—and this book—is to make students better consumers of research. So I begin by explaining the research process and the importance of critical appraisal. Thus, I begin by explaining the research process and the importance of critical appraisal. In Part I, I survey the landscape of global health research and outline the core steps of the scientific research process. In Part II, I explain how to search the literature for existing evidence, how to use filtered evidence like systematic reviews and meta-analyses, how to critically appraise scientific work, and how to evaluate claims of causality. The second main objective of my course is to make students producers of research. The rest of the book is devoted to this aim. In Part III and Part IV I address the technical aspects of global health research and cover issues related to data collection and measurement. I begin with practical advice on developing a theory of change to guide program development, data monitoring, and study evaluation. Next, theories of change and logical models are demonstrated because a good model can organize and support the approach to measurement used in a study. I then explain how to define the measurement of key study outcomes and how to use quantitative and qualitative methods to collect data on these outcomes. Thereafter, a discussion of sampling methods and sample size determination is presented. Following these basics, I turn in Part V and Part VI to researh design. This section provides a foundation that will enable researchers to evaluate their research questions to determine the most appropriate study design: experimental, quasi-experimental, or observational. Part VII is comprised of several chapters that further guide the researcher to hone the techniques within the chosen study design to ensure that the research outcomes make an impact. One limitation of this book is that I do not teach statistics. Statistical concepts are discussed throughout but not in great detail. Because statistical analysis is an intrinsic part of the study design stage, I recommend downloading a copy of OpenIntro Stats and read it alongside this book. Icons I’ve sprinkled several types of asides throughout the book: Help piecing together the global health puzzle Extended discussion of a special topic Tips Videos Interactive aids Acknowledgements I’d like to thank some folks for their helpful feedback at various points throughout my writing process. My graduate student teaching assistants, Kaitlin Saxton, Kathleen Perry, Olivia Fletcher, and Jenae Logan, read and commented on the initial drafts. This could not have been fun, so thanks! Thanks to Duke librarians Megan Von Isenburg and Hannah Rozear for setting me straight on literature searches. I still have a lot to learn! Liz Turner, biostatistican extraordinaire, kept me from making too many mistakes on technical details here and there. Gavin Yamey helped me understand what we do and don’t know about funding for global health research. I’d also like to thank students in my undergraduate and graduate global health research courses for test driving the book before all the parts were in place. Special shoutout to the following students for sharing written feedback: Kelsey Sumner, Karly Gregory, Qian Yudong, and Christina Schmidt. Despite everyone’s best efforts to help me catch mistakes, I’m certain errors remain in the book. My bad. Colophon This book is a work in progress. If you find errors (gasp!), please create an issue on Github, email me, or shame me on Twitter (@ericpgreen). I’m writing the book in R Markdown within RStudio. The bookdown package from the makers of RStudio does most of the heavy lifting to compile the book. The source code for the book is available on Github. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. References "],
["science.html", "1 Introduction to Global Health Research 1.1 Who Funds Global Health Research? 1.2 Who Produces Global Health Research? 1.3 Where is Global Health Research Published? 1.4 What Constitutes Global Health Research? Share Feedback", " 1 Introduction to Global Health Research Global health is the study and practice of “improving health and achieving equity in health for all people worldwide” (Koplan et al. 2009). The determinants of ill health and inequality are as far-reaching and complex as this definition is ambitious, and, the search for global health solutions spans many disciplines: Public health Medicine Social behavioral sciences (e.g., economics, psychology, anthropology) Biomedical engineering Biostatistics Public policy Business Global health research is multidisciplinary and interdisciplinary by design (Merson, Black, and Mills 2011). It is multidisciplinary in the sense that no one field can solve the great global health challenges of our time and interdisciplinary in the recognition that solutions can be reached through collaboration on new approaches that integrate ideas from different academic and clinical traditions. Its multidisciplinary nature means that global health research is a vast, spralling enterprise that involves many stakeholders. Let’s survey the landscape of global health research before establishing some fundamentals about scientific research. The term “stakeholders” can refer to a wide range of people and organizations. Typically, it means donors (i.e., the public and private organizations that fund research programs), policy makers (i.e., government officials and bureaucrats at national or international bodies like the CDC or the WHO), and program implementers (e.g., organizations like Doctors Without Borders) that actually deliver services to beneficiaries (i.e., individuals, populations, or patients), and it even extends to scholars who study a particular topic or policy issue. 1.1 Who Funds Global Health Research? Billions of dollars are spent on global health research every year.1 This funding comes from public institutions such as the National Institutes of Health (NIH) in the United States and private philanthropy organizations such as the Bill and Melinda Gates Foundation. In 2015, the international community disbursed $36.4 billion in development assistance for health, down from a peak of $38 billion in 2013 (Institute for Health Metrics and Evaluation 2016). Although Figure 1.1 does not present total research dollars contributed, per se, it does show the sources of global health financing, the channels through which they flow, and the areas on which they focus (see here for an interactive version).2 Figure 1.1: Flows of global health financing. Abbreviations: BMGF, Bill &amp; Melinda Gates Foundation; SWAps &amp; HSS, sector-wide approaches and health-sector support; Gavi, the Vaccine Alliance. Source: Institute for Health Metrics and Evaluation, http://vizhub.healthdata.org/fgh/ The United States Government contributed one-third of all development assistance for health in 2015 ($13 billion). More than half of USG funding was directed to HIV/AIDS, malaria, tuberculosis, and other infectious diseases. Noncommunicable diseases received &lt;1% of resources, despite accounting for 7 of the 10 leading causes of death globally (Figure 1.2). Figure 1.2: Global deaths per 100,000. Abbreviations: COPD, chronic obstructive pulmonary disease. Source: Institute for Health Metrics and Evaluation, http://vizhub.healthdata.org/gbd-compare/ 1.2 Who Produces Global Health Research? Academic centers around the world, like the Duke Global Health Institute, are major contributors to global health research. Public and private donors like the USG, the Bill and Melinda Gates Foundation, and the World Bank make grants to university-affiliated faculty members who partner with colleagues inside and outside governments to plan and conduct research. Donors also channel research support to nonprofit research organizations such as FHI 360 and RTI International, who work in a similar fashion. Interesting hybrid models include the Abdul Latif Jameel Poverty Action Lab (JPAL) and Innovations for Poverty Action (IPA). JPAL is a global network of university-affiliated professors from more than 40 universities that uses randomized evaluations (i.e., experiments) to answer policy questions related to poverty alleviation. The JPAL website, http://www.povertyactionlab.org, contains excellent resources regarding the methods used in randomized evaluations, as well as links to published studies and policy briefs. IPA is a sister organization that is also a leader in the use of randomized evaluations to study important policy questions about global poverty. 1.3 Where is Global Health Research Published? Global health research is published in medical journals (e.g., The Lancet and JAMA), general science journals (e.g., Science and PLOS ONE), discipline-specific journals (e.g., The Journal of Immunology and Epidemiology), and disease-specific journals (e.g., AIDS and Malaria Journal). Journals specializing in global health include The Lancet Global Health, BMJ Global Health, Global Public Health, and Global Health: Science and Practice. 1.4 What Constitutes Global Health Research? Like any scientific endeavor, global health research begins with basic research founded in principles of hard science. This is only the beginning, however; basic research constitutes only a small portion of the overall body of ongoing research. The areas of applied exploration built on this foundation include the clinical arenas from which important global health changes emerge. Figure 1.3: A research taxonomy 1.4.1 BASIC RESEARCH Basic research—or “pure” research—is the pursuit of fundamental knowledge of phenomena. For example, scientists conduct laboratory experiments to understand the parasitic life cycle and how parasites interact with humans at different stages. Likewise, basic ecology research seeks to understand plant species diversity. The information generated by basic science becomes the universally acknowledged background for more advanced research—applied science. 1.4.2 APPLIED RESEARCH Basic research is different than applied research, which focuses on specific problems or applications in several ways. For instance, an applied research question might be, “How can we increase the coverage and use of bed nets that prevent malaria transmission?” The basic science, such as the behavioral habits of the mosquito and the transmission conditions for malaria, have already been determined by entomologists and epidemiologists. In contrast, applied science takes many different forms, including clinical research. Clinical research is a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, outcomes research, and health services research.3 Basic research provides the foundation for all clinical research. Clinical Trials One type of clinical research is a clinical trial. Drugs and vaccines have to pass through different phases of clinical trials before regulatory bodies, such as the Federal Food and Drug Administration (FDA), will approve their use with humans: Preclinical research Phase I Phase II Phase III Phase IV Behavioral research (e.g., development and evaluation of parenting interventions) does not follow the same exact phases of vaccine and drug development, but the broad principles are the same. Case study: Developing a malaria vaccine The development of a vaccine for malaria provides a good example of the life cycle of a clinical trial. In 2015, after 30 years of research, a vaccine candidate called RTS,S, or Mosquirix™, made the news for having gotten one step closer to becoming a licensed vaccine after a successful Phase III trial. Development of RTS,S began in 1984 through a partnership between the pharmaceutical company GlaxoSmithKline (GSK) and the Walter Reed Army Institute of Research. In 1987, a promising vaccine candidate entered preclinical research. During the preclinical phase, researchers performed tests on nonhuman subjects to collect data on how well the vaccine worked (efficacy), how much damage it could do to an organism (toxicity), and how the body affected the vaccine (pharmacokinetics). Clinical research on humans began in 1992. To obtain regulatory approval, the vaccine had to complete three phases of testing. Doherty et al. (1999) conducted a Phase I safety and immunogenicity trial with 20 adults in The Gambia in 1997. This small sample size is typical of Phase I trials, where the main objectives are usually to determine a safe dosing range and to evaluate side effects. These researchers reported that the vaccine did not have any significant toxicity but did produce the expected antibodies. Several Phase II studies conducted over the following decade (Phase IIa and Phase IIb) demonstrated the efficacy of the vaccine against several end points (or outcomes) (Moorthy and Ballou 2009). A Phase IIb trial began in Mozambique in 2003 with more than 2,000 children aged 1 to 4 years (Alonso et al. 2004). Each child was randomly assigned to receive 3 doses of RTS,S or a control vaccine. After 6 months, the prevalence of malaria was 37% lower in the treatment group than in the control group. A follow-up study with 214 infants also showed partial protection from malaria (Aponte et al. 2007). This Phase II trial was an important proof-of-concept study. The final results of a large Phase III trial with more than 15,000 infants and young children in 7 African countries were published in The Lancet in 2015 (RTS,S Clinical Trials Partnership 2015). Children who participated in the study were randomly assigned to 1 of 3 arms: (a) 3 doses of RTS,S and a booster dose at month 20, (b) 3 doses of RTS,S and a booster dose of a comparator vaccine at month 20, or (c) 4 doses of a comparator vaccine. RTS,S reduced clinical malaria cases by 28% and 18% among young children and infants, respectively, over a 3–4-year period. This Phase III trial achieved its goal—to show that the treatment was efficacious. On the basis of these results, the European Medicines Agency issued a “European scientific opinion” to help inform the decision of the WHO and African national regulatory authorities regarding their recommendation of the vaccine. If RTS,S is approved for use and eventually hits the market, researchers will likely conduct Phase IV trials to evaluate the vaccine’s long-term effects. Implementation Science and Translational Research The research on RTS,S, will not end there, however. The vaccine may be efficacious, but that does not mean it will be easy or cost-effective to produce and deliver at a scale of millions. Studies that assess how best to get efficacious treatments to the people who need them most fall under the domain of implementation science. Many stumbling blocks face interventions in the process of moving from “bench to bedside.” Practitioners of translational research point to 4 key bottlenecks: T1: Translation from basic science to clinical research T2: Translation from early clinical trials to Phase III trials and beyond with larger patient populations T3: Translation from efficacy trials (i.e., Phase III trials) to real-world effectiveness through implementation science T4: Translation from evidence about delivery at scale to the adoption of new policies Figure 1.4: Source: Medical University of South Carolina, http://bit.ly/2iq2Blv Monitoring and Evaluation Another arena of applied science in global health is monitoring and evaluation, or M&amp;E. Evaluation In the United States, program evaluation became commonplace by the end of the 1950s and grew dramatically in the 1960s as the federal government expanded and introduced new social programs. Lawmakers wanted accountability, and the evaluation of social programs took off (Rossi, Lipsey, and Freeman 2003). But is program evaluation really research? Methods giant Donald Campbell4 thought so: The United States and other modern nations should be ready for an experimental approach to social reform, an approach in which we try out new programs designed to cure specific problems, in which we learn whether or not these programs are effective, and in which we retain, imitate, modify or discard them on the basis of their effectiveness on the multiple imperfect criteria available (Campbell 1969). But not everyone agrees. Some have argued that program evaluation is really designed for program implementers and funders, and that the messy nature of program implementation requires a loosening of research standards (Cronbach 1982) and simply evaluate the evidence, or “learn what you can.” In their introductory text on evaluation, Rossi et al. (2003) strike a balance in views. Their answer is perhaps a bit unsatisfying but is arguably true nevertheless: It depends. Program evaluations should be as rigorous as logistics, ethics, politics, and resources permit—and no less. Surely a lower bound in terms of quality or what is worthwhile exists, but the line is so context dependent that a simple rule is best: “Don’t go beyond the data.” Every organization wants to claim “impact,” but not every evaluation can be based on the design and implementation. Monitoring Program monitoring is concerned with the implementation of programs, policies, or interventions. How are resources being used? Is the program being delivered as intended (or with fidelity)? How many people participate, and does the program reach the intended targets? These are all program-monitoring questions. Accurate monitoring is essential for reporting data to funders, but it is also essential for all good evaluations. The reason is simple: If a program fails—that is, has no impact—the next question is why? Did the program fail because the idea or theory behind the program was wrong (theory failure)? Or was the implementation of the program so troubled that there was never a chance for success (i.e., implementation failure)? Every trial should include ongoing monitoring or a formal process evaluation to assess the impact of the program on a continual basis. Share Feedback Please feel free to add your comments about this chapter. References "],
["research101.html", "2 Research 101 2.1 Scientific Research 2.2 Stages in the Research Process Share Feedback", " 2 Research 101 At its core, global health research is based on common principles of scientific research that each discipline follows (to a greater or lesser extent), but different disciplinary traditions and emphasis can amplify what is unique over what is shared. This book is designed to teach you about what is shared while highlighting unique aspects along the way. 2.1 Scientific Research What counts as “scientific research”? King, Keohane, and Verba (1994) offer a useful definition in their book, “Designing Social Inquiry”. They point to several main characteristics: The goal is inference The procedures are public The conclusions are uncertain 2.1.1 ALL ABOUT INFERENCE By stating that the goal of scientific research is inference, we mean that science goes beyond the collection of facts. When we talk about inference, we are referring to the process of making conclusions about some unobserved or unmeasured phenomenon based on our direct observations of the world. We use what we know to infer something about the things we don’t know. This process can be deductive or inductive. In deductive reasoning, we start from general theories, make hypotheses, collect data, and make conclusions based on the data. Inductive reasoning flows the other direction, from specific observations to the generation of hypotheses and theories. Remember it this way: if you are testing a specific hypothesis, you are using deductive reasoning. If you are starting with your observations and making more general statements, then you are using inductive reasoning. To say that quantitative research is deductive and qualitative research is inductive is not quite right, but it’s often true.5 For instance, Singla, E. Kumbakumba, and Aboud (2015) report the results of a cluster randomized trial of a parenting intervention in rural Uganda. This study is an example of deductive reasoning because the authors started with a hypothesis, collected quantitative data, and inferred something about the impact of the intervention: This study used quantitative methods; the primary outcomes of this study were cognitive and receptive language development of the children of participating caregivers measured with the Bayley Scales of Infant Development. The authors hypothesized that the intervention would improve child development. They found effects on cognitive and receptive language but not height-for-age, and inferred that the difference observed between the treatment and control groups was due to the parenting intervention.6 We can contrast the Singla et al. trial with a qualitative study by Sahoo et al. (2015). Sahoo and colleagues used a grounded theory approach to conduct and analyze interviews with 56 women in Odisha, India about their sources of stress and sanitation practices.7 This study is an example of inductive reasoning because the authors started with the data—their observations—looked for themes and patterns, and came to some conclusions about the nature of sanitation-related stress.8 One result of this work was a conceptual framework for thinking about sanitation-related psychosocial stress.9 The point to take away about inference is that, regardless of the approach to reasoning, the goal of scientific research is to use what we observe to make conclusions about what we do not or cannot observe directly. This is sometimes referred to as empiricism, and our systematic observations as empirical evidence. Empiricism is at the heart of scientific research. 2.1.2 RESEARCH AS A PUBLIC ACT Scientific research uses public methods that can be examined and replicated. Replication is a core principle of scientific research. No one study rules the day. If the results of your study are robust, another research group should be able to follow your methods and replicate the findings. When findings are replicated, we all have more confidence in the results. Replications are relatively rare, however. For one, there are often few resources for replicating studies, especially when it comes to big field experiments. Second, journal space is limited (especially if there is still a print version) and peer review takes a lot of resources. Journals want to use their space and resources to publish novel ideas (ironically, novelty can sometimes mean small effects with a lot of noise that might fail to replicate). Without the promise of a publication, researchers have little incentive to spend time and money trying to replicate published findings. Publications are a key criterion for tenure and promotion in academia, so many researchers don’t waste their efforts on studies that won’t get published. What happens when replications are attempted? The short answer is bitterness. Replicators grab more headlines when they “debunk” findings, and the original authors almost invariably call into question the quality of the replication. It often leads to hard feelings on both sides. Just see #wormwars to learn what happened when a famous de-worming study was re-examined. Or Google social psychology and priming. Yikes! A related issue is reproducibility, the ability to generate a study’s findings given the original dataset and sometimes the original analysis code. Think irreproducible findings are rare? Think again. The Quarterly Journal of Political Science found that slightly more than half of their published empirical papers subjected to review had results that could not be reproduced with the author’s own code. On the positive side, it’s becoming more common for authors to share their data and analysis code. This has been standard practice in economics for some time, but the idea is pretty revolutionary in medicine and public health. We’ll explore why this is so important and easier than ever to do. 2.1.3 LIVING WITH UNCERTAINTY Every method has limitations, every measurement has error, and every model is wrong to some extent. In short, research is an imperfect process. Sometimes researchers make outright mistakes. These mistakes may or may not be detected and corrected in the peer review process, or during post-publication review if authors share their data and analysis code. Other findings are free of obvious mistakes, but fail to be replicated, and over time run counter to a growing body of literature that points in the other direction. In this way science is said to be self-correcting. We’ll discuss how this ideal can fall short in the face of challenges like publication bias, but the point here is to get comfortable in the short term with the idea of uncertainty. A good example of uncertainty comes from the estimation of maternal mortality (see Figure 2.1). Hogan et al. (2010) published estimates for 181 countries. Some countries like the United States have vast amounts of data; vital registries that attempt to track all births and deaths. Countries with vital registries struggle with changing definitions over time, but the uncertainty interval around their estimates is typically tight because there is a lot of good data. In many low-income countries the situation is very different, however. As shown in Figure 2.1, there are only four data points! No wonder the uncertainty interval is so great. The takeaway message is that there is uncertainty in everything. Don’t take any single estimate as the “Truth”. Instead, try to learn about the origin of estimates and recognize the limitations of what we know. Figure 2.1: Estimates of maternal mortality 1990-2010. LEFT: United States. RIGHT: Afghanistan. Squint and you will see that the confidence intervale for the US estimate is less than 10 out of 100,000, compared to more than 3,000 out of 100,000 in Afghanistan. Source: Hogan et al. (2010), http://bit.ly/1JBCelO So how many women die during pregnancy or within 42 days of delivery? The same research group that published Hogan et al., the Institute for Health Metrics and Evaluation, estimated that there were 292,982 maternal deaths globally in 2013, with a 95% uncertainty interval ranging from 261,017 to 327,792; that’s a range of 66,775 for everyone who struggles with mental math (Kassebaum et al. 2014). This might seem like a lot, but remember that we’re talking about global statistics for a world population of more than 7 billion people.10 2.2 Stages in the Research Process Just as you learned that every story has a beginning, middle, and end, I am here to tell you that every scientific article has an introduction, methods, results, and discussion. Follow these steps and you will have all of the pieces you need to write each section. 2.2.1 FIND A RESEARCH PROBLEM Every study begins with a research problem. A research problem represents a gap in our knowledge. In academic research, this is another way of saying a gap in “the literature”. Usually when people speak of “the literature”, they mean scholarly or peer-reviewed journal articles. There is also something called “grey literature” that is more encompassing and harder to search systematically. Grey literature sources are typically disseminated through channels other than peer-reviewed journals. Examples could include technical reports or white papers published on the web. Research problems are typically broad. For instance, stakeholders might want to know how to increase the use of bed nets for children under 5 years of age. Or whether all children should receive deworming medication prophylactically. These problems have something in common: they are solvable. In his introductory text on behavioral research methods, Leary (2012) writes that this is another key criterion for scientific research. The problems must be solvable. This does not mean easy; it just means that we can use systematic, public methods to gather and analyze data on the problem. Think of it this way: we can come up with a method for studying how to get more parents to ensure that their kids sleep under a mosquito net every night, but we don’t yet have a scientific method for determining whether there is a mosquito afterlife where these pests get to buzz around for all of eternity. 2.2.2 ARTICULATE A RESEARCH QUESTION In order to study a broad research problem, we must narrow to a more specific research question. D. de Vaus (2001) says there are essentially two types of research questions: Descriptive—what is going on? Explanatory—why is it going on? Let’s say we want to study uptake or use of bed nets. We might ask a descriptive research question like, “How many children sleep under bed nets?” But this is too general. Children of what age? Living where? We also need to operationalize what we mean by sleeping under a bed net. It’s common in this line of research to ask about the previous night, as in the night before the survey.11 A better way to phrase the question might be, “What percentage of children under 5 years of age in Kenya slept under an insecticide treated net the previous night?” An explanatory research question on the same topic might be, “What are the predictors of the use of insecticide treated net among children under 5 years of age in Kenya?” One helpful mnemonic for asking good research questions is to make them FINER: feasible, interesting, novel, ethical, and relevant (Hulley, Newman, and Cummings 2007). Feasible Some resarch questions will take a long time to answer, cost too much, require too many participants, require skills or equipment that you do not have, or will be too complex to implement. Interesting Research requires funding and effort. If you do not ask a sufficiently interesting question, you will not get funding. If you manage to get funding but lose interest in the question, you might not finish. Unlike other domains, global health research tends to have long timelines, and it’s important to work on things you will find interesting over the long term. Novel Replication is an important part of science, but the majority of funding goes to research that asks new and interesting questions. Ethical It would be very interesting to create a prison simulation to determine whether charactristics of the people or situation cause abusive behavior, but this would not be ethical because it could lead to the harmful treatment of research subjects. Right? Relevant In addition to being interesting, a research question should also be relevant. The answer should move the field forward in some way. Making this determination requires a thorough review of the literature and conversations with senior colleagues. Here’s another helpful mnemonic for creating a good clinical question: PICO. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome We could use PICO to develop a research question about the efficacy of bednets in preventing malaria. The problem is malaria infections. The population is children under 5 years of age. Intervention studies tend to be smaller in reach than nationally representative surveys, so we might add “living around the Lake Victoria basin in Kenya”. The intervention is an insecticide treated net. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] The comparison group might be children living in families who are provided an untreated bednet. One outcome could be parasitaemia. We can combine all of this into a research question: Among children under 5 years of age living around the Lake Victoria basin in Kenya, are insecticide treated nets more effective than untreated nets at preventing parasitaemia? Here is some good advice if you are writing qualitative research questions: 2.2.3 IDENTIFY RELEVANT THEORY Leary (2012) defines a theory as “a set of propositions that attempts to explain the relationships among a set of concepts”. In quantitative research, you could replace “propositions” with “hypotheses” and “concepts” with “variables”. Some studies set out to develop theory (inductive), while others may test theory (deductive). Much of applied global health is atheoretical, however. Many impact evaluations fit the label of “black box evaluations”, meaning that they don’t focus on why programs do or don’t have an impact. The evaluation is not guided by theory, and the hypotheses are as simple as “the program will have an impact on the outcome”. White (2009) outlines a strategy for changing this and moving to theory-based impact evaluations (White 2009). I talk more about this in Chapter 6 on developing a theory of change and logic model. A good resource for understanding the (potential) role of theory in global health is the journal Social Science &amp; Medicine. For instance, Green et al. (2015) frame their cluster randomized trial of an economic assistance program for women in terms of the literature on engaging men in efforts to reduce intimate partner violence. The rationale for addressing IPV through men’s discussion groups is based on the belief that socially constructed gender norms about inequality are a root cause of violence (Barker et al., 2010). Girls and boys learn gender roles and normative behavior, such as gender-based violence, by watching others and observing rewards and punishments; this is the basis of social learning theory (Bandura, 1973), one of several theoretical etiologies of IPV (for a review see Dixon and Graham-Kevan, 2011). Understanding and addressing the connection between violence and masculinity is also critical, gender theorists argue (Jewkes et al., 2014). ‘Gender-transformative’ programs are therefore designed to change gender norms and to promote gender equality among men and boys, most often by raising awareness and targeting attitudes throughout the social ecology. Search for the words “theory” or “conceptual” in the Introduction or Discussion sections of articles to see how authors frame their work in theoretical and conceptual terms. 2.2.4 DEVELOP HYPOTHESES The logical approach in quantitative research is often deductive. You start with theory and develop research hypotheses that are then tested. A hypothesis is an a priori prediction about what will occur—about how constructs are related. If the hypothesis is supported by the data, you have support for the underlying theory. If your study is well designed, it might be given more weight as other researchers consider the evidence in support of the theory. For a hypothesis to be scientific it should be falsifiable, or testable. To return to a silly example from earlier, the following would not be a research hypothesis because it cannot be tested: “if a mosquito is killed, it goes to mosquito heaven”. Maybe, but we can’t test this hypothesis. Science progresses through the possibility of falsification, so hypotheses must be engineered to potentially fail. Not all studies test hypotheses, however. Qualitative research is generally inductive and “hypothesis generating”. Without fail, students panic when they write a study proposal for the first time and get to the “develop hypotheses” step. They come to office hours all disheveled, proclaiming “I don’t have a hypothesis!” It’s OK. You might not. Can you prove a theory? Some people advocate against the free distribution of ITNs out of the belief that there is a “sunk cost” effect when having to spend money for a bed net; people will use the net more to justify their purchase (Arkes and Blumer 1985). In this case, the theory is one of sunk costs directing behavior. Cohen and Dupas (2010) designed a study to test the falsifiable hypothesis that people who paid a non-zero price for an ITN would use the ITN more than those who received the ITN for free. They did not find support for this hypothesis. So the theory of sunk costs is rejected, right? Not necessarily. Leary (2012) offers some helpful advice for thinking about proof and disproof. Proof is logically impossible, whereas disproof is practically impossible. Frustrating, right? Proof is Not Possible It helps to state the theory and hypothesis as an if-then statement. For example, “If the theory of sunk cost effects is true, then people who pay for an ITN will be more likely to use it than people who get an ITN for free.” If the theory is true, the hypothesis will be true. What happens if you flip this statement? If you find evidence that the hypothesis is true, does it mean that the theory is true? Let’s say that I have a fever and my theory is that the fever is a symptom of malaria. My hypothesis is that I must have been bitten by a mosquito. I pull back my sleeve and, voila, there are a few mosquito bites. My hypothesis was supported by data! So therefore, if my hypothesis is correct, my theory is proven, right? Well, no. I was bitten by a mosquito, but maybe the scene of the crime was my backyard in the eastern United States where we don’t worry about malaria. So in this case, the hypothesis was true, but it doesn’t prove the theory. Disproof is Possible, but Uncommon What if the hypothesis was not supported, and I was not bitten by mosquitos? Could my “theory” be true—could my fever be malaria? No. If the hypothesis is derived from the theory, and if the hypothesis is not supported, the logical inference is that the theory is wrong. Yet, we still shy away from concluding that the theory is wrong. The reason is simple: complexity. A study like Cohen and Dupas (2010) could fail to reject the null hypothesis that use does not differ between free and subsidized clients—thus not supporting the hypothesis of different use rates—but there are many practical reasons for this. For instance, maybe their measure of bed net use was systematically flawed and hid the difference as a result. The possibilities are endless. This is partly why journals are hesitant to publish null results. So what do we learn when a study does or does not support a theory? In short, no one study is enough to lead people to discard a theory. But several null results might be. Conversely, no study ever proves a theory, but an accumulation of studies showing support for the theory-derived hypothesis builds confidence in the theory. Particularly when the studies are conducted by different researchers, across different populations, and triangulating with multiple methods. Of course you see the challenge here. How do researchers know that several studies have failed to support a certain theory if journals are reluctant to publish null results? And if negative evidence is missing, won’t the positive evidence be over-represented in the literature? Yes. This is the problem of publication bias, or the file drawer problem, and there is not an easy answer. Efforts like AllTrials to register and report the results of all trials, regardless of outcome, seem like a step in the right direction. 2.2.5 SELECT A RESEARCH DESIGN As Glennerster and Takavarasha (2013) explain in their excellent practical guide to running randomized evaluations, different research questions require different research designs. The most common designs you’ll come across can be lumped into the following four categories: descriptive (can be quantitative or qualitative) correlational/observational quasi-experimental experimental It can be overwhelming for new researchers to decide on the best design to answer a particular research question. And as they say, whenever you’re feeling overwhelmed, it’s best to consult a flow chart. Figure 2.2: Research design choose your own adventure. PDF download, https://drive.google.com/open?id=0Bxn_jkXZ1lxuWkhFcTUzdWVkZ0E I introduce these designs in Chapter 5 and then spend all of Part V and VI going over them in detail. 2.2.6 IDENTIFY KEY VARIABLES A variable is something that can take on different values (Diez, Barr, and Çetinkaya-Rundel 2015). Variables can be numeric or categorical. Numeric variables can be further classified as continuous or discrete. You can add, subtract, and take the mean of continuous and discrete numeric variables. The distinction between continuous and discrete is that discrete numeric variables cannot be negative and must be whole numbers. For instance, a count of the number of mosquitos captured in a light trap is a discrete number. Conversely, the blood meal volume observed in trapped mosquitos is continuous because volume does not need to be a whole number. Continuous variables can also be classified as interval or ratio. The key difference is that ratio variables have a meaningful zero, so it’s OK to compute a ratio. For instance, if you trap 10 mosquitos and I trap 20, first, I win, but second, I beat you by a ratio of 2 to 1. Interval variables like temperature don’t have this meaningful zero. I’ve never come across other example of interval variables besides temperature, so suggest an edit if you think of one. The type of mosquito trapped is an example of a categorical variable (e.g., Anopheles, Aedes, Culex). Specifically, it’s a nominal or unordered categorical variable. If you ask someone to rate how often they’ve been bitten by one of these guys in the past week—let’s say on a 4-point scale from never to often—their response would be an example of the other type of categorical variable: ordinal. An ordinal variable is what you think: some categories are greater than others. Variable with two levels (yes and no) will often be called binary. Some disciplines such as economics also refer to categorical variables as “qualitative” variables, not to be confused with qualitative methods. Super clear, right? Variables are also classified as dependent and independent variables, depending on how you plan to use them. If you want to study the impact of ITN use on parasitaemia, parasitaemia would be your dependent variable and ITN use would be your independent variable. Or maybe you want to know what predicts ITN use. In this case, ITN use would be the dependent variable and other factors like education level would be independent variables. See below for other ways you’ll see these terms described in the literature. Dependent Variable (DV) Independent Variable (IV) Response Explanatory Outcome, Endpoint Predictor, Risk Factor Y X Qualitative researchers do not typically talk about the measurement of dependent and independent variables, but rather concepts and constructs. This is because in qualitative research, the goal is not to test a hypothesis that some independent variable predicts some dependent variable. Instead, the goal is to explore some phenomenon and describe it in as much detail—thick description—as possible. Some researchers will quantify qualitative data, however, so it’s not necessarily a number free zone (e.g., frequencies). 2.2.7 SELECT APPROPRIATE RESEARCH METHODS If research designs are strategies for answering research questions with the best possible evidence, then research methods are the tactics for obtaining the evidence (Chapter 8). Often methods are divided into three broad categories: quantitative qualitative mixed Quantitative methods are used to collect and analyze numerical data. This includes binary or dichotomous** data (e.g., hospitalized or not), categorical data (e.g., wealth quintile), and continuous data (e.g., hematocrit). A good example of a quantitative method is a survey in which people are asked to answer questions with fixed response options or provide numerical values, such as their monthly income. Lab tests resulting in disease classifications (yes/no) or a measurement such as the number of blood cells in a sample of blood are also examples of quantitative methods. Qualitative methods focus on non-numerical data. Participant observation, interviews, and focus group discussion are common qualitative methods in global health. Qualitative methods are well-suited for obtaining thick description and for exploration. Often qualitative methods are seen as being less rigorous because they are more flexible and do not lead to the same type of hypothesis testing and results compared to quantitative methods. But this is not true. As we’ll discuss in a later chapter, rigor is a characteristic of how the methods are applied rather than the methods themselves. Your choice of methods should be based on your research question. It’s often the case that impact evaluations use quantitative methods, but there is not a 1-to-1 match between research designs and methods. Many studies incorporate both quantitative and qualitative methods, and we refer to this as mixed methods. Sometimes the goal of mixing methods is triangulation of results with respect to the same research question. Other times we begin with qualitative work to develop the tools and measures that we will use in a trial. When qualitative work follows a quantitative phase, the goal is often to explain or explore results in more depth that was not possible with the quantitative data. Increasingly you will see RCTs complement their use of quantitative methods with qualitative inquiry (O’Cathain et al. 2013). Alaii et al. (2003) provide a good example. The authors of this paper incorporated qualitative interviews on non-adherence into a larger randomized trial of the efficacy of ITNs on child morbidity and mortality in Kenya (Phillips-Howard et al. 2003). They wanted to better understand why people, particularly children under the age of 5, were not using their ITNs correctly. Alaii et al. found that more than a quarter of individuals were non-adherent, often due to excessive heat. 2.2.8 SPECIFY AN ANALYSIS PLAN This is not a book about data analysis, but it’s important to note that a pre-specified analysis plan is an important component of qualitative and quantitative research proposals. On a practical level, you need to think through your plan to make sure that the study you’ve designed will produce the data you need to carry out a specific analysis. More generally, however, pre-specified and registered analysis plans help to promote transparency and confidence in study results. The basic reason is that researchers make tens or hundreds of small decisions during the course of data processing and analysis that can alter the results. I’m not even talking about fraud. I’m talking about legitimate, defensible decisions. The problem occurs when these decisions are made after seeing the data. For instance, a researcher might run a test and find that a relationship is not statistically significant. The researcher then makes a small change in how a variable is defined, and runs the test again. This time the relationship is significant. Whoohoo! Publication! Tenure! Not having to move your family to a new state! The following video is a great introduction to some of these thorny issues that we’ll tackle in a later chapter. For now, I’d just like to convince you of the benefits of pre-registration, or publishing your analysis plan in advance of getting the data. If you’re working on drug trials, you might not have a choice if the research is regulated by the FDA; you’ll be required to register the study on clinicaltrials.gov. If you’re working in another area you might not have a regulatory requirement to register your study, but your journal of choice might require that you do so in order to publish the results. 2.2.9 OBTAIN ETHICAL APPROVAL Research involving human subjects must be reviewed and approved by an institutional review board (IRB) prior to commencing. According to the U.S. Department of Health &amp; Human Services, 45 CFR 46, “research” is defined as a systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge. As shown in Figure 2.3, there are several categories of research with human subjects that are designated as exempt. Figure 2.3: Is the human subjects research eligible for exemption?; Source: http://bit.ly/2brlbKR. Unless your study is exempt from review or meets the requirements for expedited review, you should expect to have your proposal reviewed by the full committee at a regularly scheduled IRB meeting. If you are conducting international research, you likely have to leave sufficient time for your proposal to be reviewed by an in-country IRB in addition to your home institution’s IRB. “Sufficient” could mean 6 weeks or 6 months, so you must start early. Global health research is collaborative by design and necessity, so rely on the local expertise of partners to advise on IRB procedures. 2.2.10 RECRUIT A SAMPLE AND COLLECT DATA Unless you are planning a secondary analysis of existing data (e.g., medical record review), you must identify a sampling strategy and outline procedures for data collection. I cover these topics in Chapters 9 and 8, respectively. 2.2.11 ANALYZE THE DATA AND WRITE UP THE RESULTS Complex global health research typically involves specialists in data analysis, most commonly biostatisticians. It is wise to consult with these experts early in the process of designing a study so you can ensure that you will have the raw materials you need for your planned analysis. Writing manuscripts in global health is a collaborative process. Some disciplines like economics tend to have very few authors. Some medical studies conducted at multiple sites can have dozens. The International Committee of Medical Journal Editors suggests that authors be defined by four criteria: Substantial contributions to the conception or design of the work; or the acquisition, analysis, or interpretation of data for the work; AND Drafting the work or revising it critically for important intellectual content; AND Final approval of the version to be published; AND Agreement to be accountable for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. Any contributors who do not meet these criteria should be acknowledged. Author order is handled differently by different disciplines. In psychology, for instance, the author listed first is supposed to be the “lead” author who contributed the most. The last author could be the person who contributed the least, or it could be the senior-most member of the “lab” that produced the work. In economics, it pays to be Prof A. rather than Prof C. because author order tends to be alphabetical, but not always. For senior researchers author order may be an afterthought, but junior scholars need to establish a record of first author publications to signal their emergence as an independent scientist. 2.2.12 MAKE YOUR RESEARCH HAVE AN IMPACT It’s not uncommon in global health for an intervention study to span 5 to 10 years from proposal to final publication in the scientific literature. And there is an important distinction to make between publication and impact. While there are metrics to track the impact that journals, articles, and authors have on a field, we can also think about impact in terms of real-world change. Does your work lead to new policies or programs? Does it change the way that people think? Is your work used—or if we want to be fancy, “utilized”? We’d like to think that if you just do good work, others will take it up. In reality, however, there is a gap between research and practice/policy. Many ideas—p-values less than 0.05—are sitting on the shelf collecting dust. In response, some have advocated for a research utilization framework to promote the advance planning that is needed to engage the potential users of research before the study even collects any data. Figure 2.4: Research utilization framework. Source: http://bit.ly/2j1dLfL Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["literature.html", "3 Searching the Literature 3.1 Start with Systematic Reviews and Meta-Analyses 3.2 Devising a Search Strategy 3.3 For the Love of Everything Holy Use a Reference Manager 3.4 Why Does Any of This Matter? Share Feedback", " 3 Searching the Literature The starting point of every research study is a literature review. To know where you are going, you need to know where the field has been. Technology makes this easier in some ways than it has been in the past, but we’re swimming in information, and the pool gets deeper every day. A lot deeper, actually. Google’s former CEO Eric Schmidt has said we create as much information every two days as we did from the beginning of time through 2003. Two days! And he said this back in 2012, so it’s an even shorter time span today. Of course the “cat photo” to “research finding” ratio is probably something like 1,000,000:1 nowadays, but this only makes the point that good information can be hard to find. In this chapter we’ll discuss a strategy for quickly getting a sense for the state-of-the-art in health research, and then outline the steps you need to take to search the literature for primary sources. 3.1 Start with Systematic Reviews and Meta-Analyses PDF slide deck Repeat after me: I will not start my research by Googling “malaria”. I will not start my research by Googling “malaria”. I will not start my research by Googling “malaria”. If your topic is malaria and you’re not sure if the vector is mosquitoes or monkeys, then Wikipedia is probably a good place to start. There’s no shame in that. Otherwise, it is always a good idea to begin with a check for relevant systematic reviews or meta-analyses. Finding a good review beats Googling “malaria” any day. 3.1.1 META-ANALYSIS A meta-analysis is a quantitative approach in which the results from multiple studies are combined to estimate an overall effect size. The results of a meta-analysis are typically summarized in a forest plot like the one shown in Figure 3.1. Let’s take a look at this helpful guide from Ried (2006) that breaks it all down. Figure 3.1: Source: Ried (2006), http://bit.ly/2j9pfSz A forest plot summarizes the results of several studies that measured the effect of a particular intervention on a particular outcome. Each study result is described and plotted in one row, and the overall (aka the “pooled” or “meta”) effect that takes all of the studies into account is displayed at the bottom. The study sample is divided into the intervention and control arms and presented in the n/N format where n represents the number of participants who experienced the outcome and N is the total number of participants in the study arm. For example, there were 141 people assigned to the intervention group in Study A. 1 out of these 141 people experienced the adverse outcome that the forest plot summarizes. Next comes a plot of the effect size and the confidence interval. An effect size is a measure of the strength or magnitude of a relationship, such as the relationship between taking a medicine and experiencing a bad outcome. This guide shows one particular type of effect size: relative risk. Each study’s point estimate of the relative risk is plotted around a line of “no effect”. A risk of 1 means that there is no difference between the intervention and control groups. When the outcome is something bad, like death, you want your intervention to reduce the risk, which is a risk ratio less than 1. We talk about “estimates” of the effect because research can only approximate the truth. Recall from our discussion of scientific research in the previous chapter that this is the concept of uncertainty. Forest plots show uncertainty in the form of confidence intervals. The size of the effect estimate symbol is based on how much the study contributed to the meta-analysis. All studies are not created equal, and the weight parameter lets researchers reflect this in the analysis. Each point estimate is surrounded by a confidence interval (typically 95%) that is summarized numerically in the final column. I’ll cover effect sizes and confidence intervals in a later chapter, but the basic idea is that if you were to conduct the study 100 times, you’d expect the effect size to be in this interval 95% of the time. The key point to grasp now is that when this interval crosses the line of no effect, we have to acknowledge that the effect could be null or even run in the opposite direction. We’d say that the result is not “statistically significant”. Finally, note the test for heterogeneity toward the bottom. Heterogeneity means diversity (and is the opposite of homogeneity). Heterogeneity in a forest plot refers to the diversity in effect size estimates across studies. Heterogeneity complicates the interpretation of a meta-analysis; it signals that we might be comparing apples and oranges. For instance, maybe the intervention works differently in different contexts, and the included studies come from all over the world. In such a case, it might not make sense to try to come up with one, overall, meta effect size from a comparison of apples and oranges. The first way to assess heterogeneity is to just look at the plots. Do the confidence intervals from each study form a vertical column, even if the point estimates shift within? If so, heterogeneity is probably low. You can also summarize heterogeneity numerically. Two estimates of heterogeneity are often presented: Chi-square and I^2. I^2 is generally seen as the preferred metric. Values greater than 75% might lead you to change how you run your meta-analysis (random vs fixed effects). The details are not important now; just know that if you see a high I^2 reported, you should expect the authors to address heterogeneity in the methods or limitations. Lewis and Clarke (2001) discovered that the first forest plot was published in 1978, and first used in a meta-analysis in 1982. The name lagged behind, appearing first in 1996, apparently referring to the forest of lines typical of most forest plots. Example Let’s consider a meta-analysis by Radeva-Petrova et al. (2014) as an example. The authors reviewed 17 studies of the effects of chemoprevention on pregnant women living in malaria-endemic areas. The basic question they set out to answer with their review was as follows: Do women who take antimalarial medication during pregnancy have a lower risk of getting infected with malaria, and thus a lower risk of experiencing the bad health outcomes that are associated with malaria? One indicator of malaria infection is parasitaemia, or the presence of malaria parasites in the blood. If chemoprevention has some preventive effect, you’d expect to see less parasitaemia among women exposed to the medication (aka, treatment). Few interventions are 100% effective, so we often talk about reductions in the risk of bad outcomes like malaria. The forest plot shown in Figure 3.2 displays the results of 10 studies (8 trials) that compared cases of parasitaemia among 3,663 pregnant women who were randomized to an intervention group (n=2,053) that received some preventive antimalarial drug or to a control group (n=1,610) that received a placebo (or nothing). Figure 3.2: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Details about each study are reported as rows in this figure. Take a look at the study by Shulman et al. (1999) in row 6. This study found that 30 of the 567 women in the intervention group tested positive for parasitaemia (i.e., malaria). This compared to 199 of the 564 woman in the control group. This is a risk ratio of 0.15—(30/567)/(199/564) = 0.15—which means that chemoprevention reduced the risk of parasitaemia by 85%. This is a huge effect size! The effect size for each study is presented in the far right column and depicted graphically in the size of point estimate square. All of the point estimates fall to the left of the line of no effect (&lt;1), thus favoring chemoprevention because you want to reduce the risk of parasitaemia. [A risk ratio of 1 would indicate no difference in risk, and a ratio &gt;1 would mean the risk is higher among the intervention group, thus favoring the control group.] The overall (pooled) effect size is shown last as 0.39, or a 61% reduction in the risk of parasitaemia. We won’t bother ourselves with the calculation of this pooled effect size, other than to note that it’s not as simple as averaging the 10 studies. This is because the studies were not given equal weight, as you can see in the “weight” column. For instance, Greenwood et al. (1989) only had a sample size of 21+13=34 children. As a result, the effect size estimate is very noisy. The 95% confidence interval is wide and crosses 1. Consequently, its weight of 6.7% is lower than the others. Simply put, studies weaker research designs get less weight in the analysis. So here in one figure we get a summary of the best available evidence and an estimate of the overall effect size, with uncertainly intervals. You can’t get that from a Google search. Exploring meta-analysis 3.1.2 SYSTEMATIC REVIEW You might be wondering how Radeva-Petrova et al. (2014) found these studies in the first place. The answer is through a systematic review of the literature. Most, if not all, meta-analyses will be completed as part of a systematic review of the literature, and every systematic review is a type of literature review. But not every literature review is a systematic review, even if done systematically. Figure 3.3: Literature reviews, systematic reviews, and meta-analyses As you can see from the table below, a systematic review requires a number of steps that are good practice, but too thorough and time-consuming for the general literature review you might prepare when starting your work. Nevertheless, you need to know the general process of preparing a systematic review to evaluate the quality of the reviews you read, and you’ll hopefully pick up some good habits along the way. Table 3.1: Comparing systematic reviews and literature reviews. Systematic Reviews Literature Reviews The goal of a systematic review is to be comprehensive and include every relevant article. The literature review that you write for the introduction of your manuscript is not expected to be exhaustive. For this reason, most systematic reviews are conducted by teams given the large scope of the work. literature reviews can be handled solo. Systematic reviews must define and follow a method that can be replicated, just like any other study. Literature reviews, on the other hand, don’t have to follow such rigid methods or make the methods explicit. Most systematic reviews pre-register this plan, meaning that the authors submit their planned methods to a registry like PROSPERO prior to conducting the study. Pre-registration gives other researchers confidence that the team is not cherry picking results at the end to make an interesting paper. It also lets other researchers know that a group is already working on the same review, thus signaling that their work might duplicate efforts and fail to get published. Not the case for literature reviews. Included in these pre-registration plans will be a specific search strategy with exact search terms for individual scholarly databases so other researchers can recreate the search. It’s a good idea to do the same for a literature review, even if not a strict requirement. Similarly, a systematic review must also outline clear criteria for including and excluding studies (e.g., keep if assignment to study arms was random). With these criteria in place, team members screen all search results, usually starting with title and abstract reviews only and moving to full text reviews as the pool of eligible studies dwindles. Screening for a literature review is typically less intensive. Systematic reviews also develop and follow guidelines for extracting details from every included study, such as numbers of participants and key outcomes. An annotated bibliography might suffice for a literature review. Finally, teams conducting systematic reviews formally assess the quality of each included study, including the potential for bias, and take these assessments into account when synthesizing the results. This process is more ad hoc for literature reviews. Where to find systematic reviews Three excellent sources for finding systematic reviews (and meta-analyses) in global health are the Cochrane Library, the Campbell Collaboration, and 3ie. You can also get to many of the reviews in these databases by searching within PubMed using the Clinical Queries feature. How to read systematic reviews Abstract and plain language summary Cochrane reviews follow a standard format that can look overwhelming at first, but is actually quite easy to read and understand. As with most journal articles, Cochrane reviews begin with an Abstract. Next comes a Plain language summary which can be helpful for newcomers to a particular topic. Radeva-Petrova et al. (2014) include the following passage in their plain language summary: For women in their first or second pregnancy, malaria chemoprevention prevents moderate to severe anemia (high quality evidence); and prevents malaria parasites being detected in the blood (high quality evidence). It may also prevent malaria illness. We don’t know if it prevents maternal deaths, as this would require very large studies to detect an effect. This one paragraph brings us up to speed with the state of the science for preventing malaria and its effects among pregnant women living in malaria-endemic areas (and points you to some gaps in the literature!). Google does not filter the evidence in this manner. Starting with systematic reviews pays off almost every time. Summary tables Next come the Summary tables, such as the one presented below from Radeva-Petrova et al. (2014). These tables round out everything you need to make your initial judgment. Figure 3.4: Malaria chemoprevention for pregnant women living in endemic areas. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj The first comparative risk column shows the assumed risk among the control group. For instance, the risk of antenatal parasitaemia is 286 events per every 1,000 people. This is the median control group risk across eight trials of 3,663 women. The relative risk is 0.39—recall that this is the pooled, or “meta” effect size—so you can see how the corresponding risk among the intervention group is 286*0.39=111 per 1,000 people.12 As shown in the final column, the quality of this evidence is rated as “high”. The authors are referring here to GRADE criteria, a systematic approach to evaluating the quality of empirical evidence: High—Further research is very unlikely to change our confidence in the estimate of effect. Moderate—Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate. Low—Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate. Very Low—We are very uncertain about the estimate. Background Much of what you want to know you can learn from the abstract, summary text and tables, and forest plots (if included). If you keep reading, you’ll come next to the Background section. This is typically a short overview that explains what gaps in our knowledge the review is intended to fill. Radeva-Petrova et al. (2014) use this section to present a conceptual framework for malaria prevention during pregnancy. Figure 3.5: Drugs for preventing malaria in pregnancy: conceptual framework. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Methods The Methods section details how the review was organized and conducted. The purpose of this section is to provide enough detail to enable other researchers to attempt to replicate the review. The main components are:13 A description of the population and intervention. The key outcomes of interest. The search strategy and databases. Inclusion and exclusion criteria for studies. Procedures for extracting information from each study. Procedures for assessing bias and conducting a meta-analysis (if one is included) Results The Results section typically begins with details about how many primary articles were identified, screened, and excluded, typically presented graphically with a flow diagram like the one below from Radeva-Petrova et al. (2014). Figure 3.6: Study flow diagram. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Once the included studies are identified, it’s customary for review authors to report on the quality of the evidence presented in each study. We’ll discuss the nature of these sources of bias in a later chapter, but you should familiarize yourself with these heatmaps. While a bit on the ugly side, they provide a useful summary of bias. As a future producer of research, you should start to look at these figures and think, “how can I make sure my studies are full of green pluses?” Figure 3.7: Risk of bias summary: review authors’ judgements about each risk of bias item for each included trial. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj Discussion and conclusions Discussion sections provide a short summary of the findings, commentary on the quality of the evidence, and thoughts about what the review adds to the existing literature on the topic. A discussion tends to be short relative to the size of the overall review. Discussion sections are often followed by a brief statement of the authors’ conclusions. This is an opportunity for the authors to frame the results in terms of the implications for practice and research. Radeva-Petrova et al. (2014) conclude: Routine chemoprevention to prevent malaria and its consequences has been extensively tested in RCTs, with clinically important benefits on anemia and parasitaemia in the mother, and on birth-weight in infants. In other words, “chemoprevention works” in this context. Appendices Reading the appendices will give you a sense of what it takes to put together a systematic review. There are usually tables after tables of characteristics of included and excluded studies, often followed by dozens of forest plots if the systematic review includes a meta-analysis with several outcomes or populations of interest. Radeva-Petrova et al. (2014) wrap up on page 120! 3.2 Devising a Search Strategy Hopefully at this point you’ll agree that it’s a good idea to start with a systematic review, not a search engine. Of course not every topic has been the subject of a recent systematic review or meta-analysis, so you’ll sometimes need to search the primary literature yourself. I’ll show you how, but first you need to clearly define what you’re looking for. 3.2.1 ASKING A RESEARCH QUESTION Let’s come back to our helpful mnemonic PICO introduced in the previous chapter. PICO can help you create a good clinical question. P Patient, Population, or Problem I Intervention, Prognostic Factor, or Exposure C Comparison O Outcome We’ll use PICO to develop a well-focused, searchable research question on treating malaria during pregnancy. The problem we want to address is malaria infections. The population is pregnant women living in malaria-endemic areas. Not every clinical question involves testing of a treatment or intervention, but we’ll focus a lot on these types of questions in this book. For the example at hand, the intervention would be malaria chemoprevention. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.] Similarly, not every question involves a comparison group.14 In this example, the comparison is nothing or a placebo. There are many potential outcomes for treating malaria. In this case, let’s focus on parasitaemia. We can combine all of this into a research question: Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Remember, not all research questions fit into PICO. That’s OK. Go back to Chapter 2 to remember how to make your questions FINER. 3.2.2 APPROACHES With your basic research question outlined, you’re ready to begin searching. At the beginning you might take a quick and dirty™ approach to get started. Eventually you’ll need to graduate to a proper search strategy to be more systematic, even if your end goal is not a capital “S” Systematic review. Quick and dirty™ A reasonable initial approach is to find a few recent articles to get a quick sense of what is out there. Google Scholar could come in handy here. For instance, my advanced Google Scholar search for PICO terms “malaria pregnant chemoprevention parasitaemia” (limited to recent years) identified a paper by Braun et al. (2015) on the use of intermittent preventive treatment in pregnancy (IPTp) with sulfadoxine–pyrimethamine (SP)—a specific type of chemoprevention—on malaria infections among pregnant women in western Uganda. Customize your Google Scholar experience by clicking on the gear icon. Enable use of a bibliography manager, and click on “Library links” to add your library to get links to full text. A good starting point for future searching is to note an article’s keywords. Not all journals print keywords, but if they do, you’ll probably find them right after the abstract. Next comes the introduction. Some journals and disciplines have very brief introduction sections and might not be of much help. This is often true in medicine and public health. The discussion section is also a place to look for new leads. Authors typically use the discussion to link the study results to the existing literature, demonstrating how the results add to what is already known. After looking at the introduction and discussion sections, it’s often useful to skim the references to get a sense of which journals publish this type of work. If a certain journal appears to be a common outlet for this work, a scan of the journal’s table of contents for recent issues could be useful.15 If you have access to a university library, you can learn more about the scholarly journals in a field by looking up Journal Citation Reports. This annual report ranks the journals in each field according to impact factors. Impact factors are one metric used to evaluate the importance of a journal in its field. More systematic Plan and document your search strategy Whether or not you are conducting a capital “S” Systematic review, it’s a good idea to plan and document your search. You don’t need to be as thorough in a lit review as you would for a systematic review, but it wouldn’t hurt to take a page from the approach. Let’s look at Radeva-Petrova et al. (2014) for some inspiration. Every good systematic review will include a table or appendix like this one to make the method reproducible. If you and I run this search query at the same time on two different computers, we should get the same results. Figure 3.8: Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj For the purposes of your literature review, you don’t necessarily need to ensure that other people can recreate your results, but you should make sure that you can 2 weeks from now. Create an account within the database you are searching to login and save your search approaches and make it easier to retrace your steps at a later date. Also, differences in the design of each database and interface often require you to customize your search. If you are conducting an actual systematic review that you wish to publish—as opposed to just searching the literature systematically—then you should consult with a librarian who will be familiar with the intricacies of building search strategies. Selecting a database As you can see from the table, Radeva-Petrova et al. (2014) searched five databases. MEDLINE is probably the most well known of this group. When you search PubMed, you are searching the MEDLINE database. This is typically a good place to start to find health-related studies. Talk with a research librarian to understand if other databases might be a better choice for your topic. Generate search terms Once you decide on a database, you need to generate search terms. Start with the keywords published with the sample articles you dig up. You can learn a lot about potential keywords by searching for MeSH terms. MeSH, which stands for “Medical Subject Headings”, is a controlled vocabulary thesaurus that is used to index articles in PubMed. This thesaurus is helpful because there are often many ways to refer to the same phenomenon. For instance, the MeSH term for “breast cancer” is “Breast Neoplasm”. When you search for “breast cancer” in PubMed, the database helps you out by casting a wider net by including the MeSH term automatically behind the scenes (note: it doesn’t always do this! You can check up on what PubMed includes in the search by clicking on the search details link after running a search): &quot;breast neoplasms&quot;[MeSH Terms] OR (&quot;breast&quot;[All Fields] AND &quot;neoplasms&quot;[All Fields]) OR &quot;breast neoplasms&quot;[All Fields] OR (&quot;breast&quot;[All Fields] AND &quot;cancer&quot;[All Fields]) OR &quot;breast cancer&quot;[All Fields] Turns out there are a lot of ways that we refer to breast cancer! The following entry terms are indexed to the MeSH term “breast neoplasms” by humans at PubMed: Breast Neoplasm Neoplasm, Breast Neoplasms, Breast Tumors, Breast Breast Tumors Breast Tumor Tumor, Breast Mammary Neoplasms, Human Human Mammary Neoplasm Human Mammary Neoplasms Neoplasm, Human Mammary Neoplasms, Human Mammary Mammary Neoplasm, Human Mammary Carcinoma, Human Carcinoma, Human Mammary Carcinomas, Human Mammary Human Mammary Carcinomas Mammary Carcinomas, Human Human Mammary Carcinoma Breast Cancer Cancer, Breast Cancer of Breast Mammary Cancer Malignant Neoplasm of Breast Malignant Tumor of Breast Breast Carcinoma Cancer of the Breast Back in the world of mosquitoes, the MeSH term for “malaria” is “malaria”, conveniently, and a search for this term in PubMed actually searches: &quot;malaria&quot;[MeSH Terms] OR malaria[All fields] The following entry terms are indexed to the MeSH term “malaria”: Remittent Fever Fever, Remittent Paludism Plasmodium Infections Infections, Plasmodium Infection, Plasmodium Plasmodium Infection Marsh Fever Fever, Marsh Running your search Once you have some initial search terms, it’s time to build and run your query. This will be an iterative process, full of trial and error. You might start with 200,000 results. Some terms and combinations will fail to narrow this field. Others will trim too much. Figure 3.9: Boolean operators: AND, OR, NOT You’ll need to know some basic Boolean operators to be an effective searcher: AND, OR, NOT. For instance, let’s consider the search PubMed runs when you enter “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) OR (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) These four terms are combined with OR, meaning we keep results that match any of these terms. At the time of writing, PubMed returns 922,588 results. Of course it would make more sense to search for “malaria AND pregnancy”, instead of “malaria OR pregnancy”: (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) The first two terms and last two terms are combined separately with OR. These combinations are then combined with AND (notice the use of parentheses to segment the operations), dropping our pool of results to 4,203 records. The AND operator will always maintain or decrease the number of results. If we want to limit the results humans, we can add AND &quot;humans&quot;[MeSH Terms] to the end.16 Doing so drops our pool of results to 3,798. (&quot;malaria&quot;[MeSH Terms] OR &quot;malaria&quot;[All Fields]) AND (&quot;pregnancy&quot;[MeSH Terms] OR &quot;pregnancy&quot;[All Fields]) AND &quot;humans&quot;[MeSH Terms] Alternatively, we could use the NOT operator to limit the results to non-humans, but NOT is not commonly used. No it’s not. Let’s return to our PICO question and use Boolean operators to combine the components. Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Here’s what we want to do in plain English: pregnancy OR pregnant women AND malaria endemic AND chemoprevention (if you know specific drugs to search, string together with ORs) AND randomized controlled trial AND parasitaemia As I tap out these words on my keyboard, this search returns 513 records in PubMed. Once you are satisfied with your results, you could choose to apply the same search to another database. This might be worth the effort if your topic crosses disciplinary boundaries, like economics and health. Best to check with a research librarian. Oh hey, here’s one now: Screening results Even the best search queries return some duds, so the next step is screening. We can return to Radeva-Petrova et al. (2014) to see what a thorough approach looks like. You would likely take some shortcuts for a regular literature review. Typically systematic review searches will return hundreds or thousands of potential hits, so a study team will screen titles and abstracts to exclude obvious mistakes. When beginning this process, it’s common to have team members screen some of the same records to establish reliability, a concept that we will discuss in more depth in the chapter on measurement. Basically, you want to know that everyone screening records would make the same inclusion/exclusion decision. The Radeva-Petrova et al. (2014) search strategy turned up 179 unique records, and the authors excluded 126 of these records after screening the abstracts. The excluded studies did not meet certain pre-defined criteria. For instance, the authors only wanted to include studies using RCTs and quasi-experimental designs. This left the team with 53 studies that required a full-text review. Only 17 of the 53 studies still met eligibility criteria after this review.17 Supplemental searches It is customary in a systematic review–and helpful in general reviews—to augment database searches with reference reviews and hand searches to ensure that no key references were missed in the database query. A reference review is nothing more than a scan of an eligible article’s bibliography. In a hand search, you would go to the website of journals that published the eligible articles and scan the tables of contents for each issue published during the search window. If you find that either supplemental method turns up a lot of new results, it could make sense to revise your systematic review search strategy to be more comprehensive. Extracting data Depending on your objectives you might choose to systematically extract data from each study—key facts related to study design, methods, and results. Or you might take a shorter path and create an annotated bibliography. If you need to be more systematic—an essential requirement for a capital “S” Systematic review—then you need to design a data extraction strategy. Your PICO research question can be a helpful guide to identifying the minimum data you should extract. Returning to Radeva-Petrova et al. (2014): Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia? Some possibilities include: study setting/population sample size sample demographics, including parity study design intervention details, such as specific medication and dose primary outcome (e.g., parasitaemia) effect size There are numerous software options for storing your extracted data, but you’ll likely find that a simple spreadsheet with rows of studies and columns of study variables will work just fine. Lots of teams use this approach for big systematic reviews, so it will probably serve you well for something more modest. 3.3 For the Love of Everything Holy Use a Reference Manager Even if you chose to ignore everything I’ve written up to this point, do yourself a favor and use a program for managing references. I’m amazed every year when I learn that students on the precipice of graduation manually type and format their in-text citations and bibliographies. What a waste of time! There are several reference managers you might consider. I’ll mention one because it is free and open-source: Zotero. The concept of “free” does not need much explanation, but students often have several free options that are not really free. A good example is a program like EndNote. A university might make this program a free download for enrolled students, but the license expires upon graduation or soon becomes obsolete without a paid upgrade. Additionally, in global health it’s common to work with colleagues who do not have access to a program like EndNote, which makes collaboration challenging. For these reasons, I highly recommend a program like Zotero that is free to use and open to improve. A tutorial is beyond the scope of this chapter, but it’s worth mentioning some features that are common to many reference managers: Easy importing of references from databases like PubMed. Go from your search results to reference manager in seconds. Automatic retrieval of full-text PDF. Sync PDFs in collection to tablets and phones Connections to word processing software to make inserting references in papers a snap. Automatic creation of bibliographies based on works cited. Push button reformatting of in-text citations and references to different styles, such as APA and Harvard. Shared collections with automatic syncing via the cloud to facilitate collaboration. Easy export of references for migration to just about any other reference manager. So next time you see someone typing references and complaining about APA formatting, open your laptop and run your reference manager. Watch them weep! 3.4 Why Does Any of This Matter? Most of the time we don’t search the literature to conduct our own systematic review. We search the literature to keep up with developments in our field and to appreciate where there are gaps in our collective knowledge. We search the literature to write FINER research questions. Questions that are Interesting, and Novel, and Relevant. We search the literature to understand how people in our field conceptualize study designs, plan study measurement, and report results. It takes practice to know how and where to search and to quickly evaluate and digest studies. Especially when you are just starting out, effective search strategies can help you avoid the rabbit holes that someone more experienced would ignore. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["critical.html", "4 Critical Appraisal 4.1 Be Skeptical of News Reports and Press Releases 4.2 Peer-Reviewed Does Not Mean “Correct” 4.3 How to be a Good Consumer of Research Additional Resources Share Feedback", " 4 Critical Appraisal Whether you are a clinician treating patients, a public health official making policy recommendations, or a representative of a donor agency responsible for setting funding priorities, your decisions should be based on evidence, not beliefs. The phase “evidence-based” first came into use in the 1990s, nearly two decades after the likes of Archie Cochrane highlighted how medical practice lagged behind what science had to say about “what works”. Medical schools soon began teaching their students evidence-based medicine (EBM), defined by Sackett et al. (1996) as: The conscientious, explicit and judicious use of current best evidence in making decisions about the care of the individual patient. It means integrating individual clinical expertise with the best available external clinical evidence from systematic research. From there we witnessed an expansion of the “evidence-based” idea to evidence-based practice more generally, as well as to population-level approaches such as evidence-based public health (Brownson, Fielding, and Maylahn 2009) and evidence-based global health policy (Yamey and Feachem 2011). Just as in EBM, the aim of these newer conceptualizations is to adopt policies and programs that have been shown to save lives and improve health at scale. But before policymakers can make evidence-based decisions, the scientific community must evaluate the strength of the evidence. This is commonly referred to as critical appraisal, the topic of this chapter. 4.1 Be Skeptical of News Reports and Press Releases Much of what we learn about scientific results from the media comes in the form of clickbait, such as this article in Discover Magazine titled “Want to avoid malaria? Just wear a chicken.” Would you be surprised if I told you that the study did not actually require people to wear chickens?18 Are you worried about getting malaria? Well, according to this study, you might be able to avoid it by carrying a chicken everywhere you go. These findings…give “tastes like chicken” a whole new meaning! Despite the outlandish title, this news report got it mostly right, so it’s better than most. If you turn on the news or read a university press release, you’ll often find summaries and claims that go far beyond the conclusions of the original article. This is because most studies give us a small glimpse at the “truth”, but the measured and careful language of scientific articles does not always capture the attention of the public. Part of the problem is also that good science writers are hard to come by. One of the best is Ben Goldacre, a British psychiatrist who runs the EMB Data Lab at the Centre for Evidence-Based Medicine at the University of Oxford. He wrote the Guardian’s “Bad Science” column for a decade, and later published a great book with the same title. Check out his more recent work on AllTrials. 4.2 Peer-Reviewed Does Not Mean “Correct” Journalists and communications professionals can and do make mistakes in summarizing and interpreting scientific findings, but sometimes the problem is further upstream with the study authors. But how can published research be flawed, you ask? It was peer-reviewed! Peer review is an important component of the scientific process, but it is not a guarantee of “truth” or a certification of the results. So what is peer review? Exhibit A for peer review does not mean “correct”: a Lancet article published by Pronyk et al. (2012) about the impact of the Millennium Villages Project (MVP) on child survival in rural sub-Saharan Africa. The MVP was founded by economist Jeffrey Sachs as a proof of his concept that extreme poverty could be solved, and the Millennium Development Goals met, with a big financial push. The basic idea behind a Millennium Village is to intervene across sectors simultaneously—water, sanitation, education, health, etc.—to give communities a chance to escape the poverty trap. Sachs and his colleagues published their first comprehensive report on the MVP in the Lancet—a peer-reviewed journal—with the stated aim to “assess progress towards the MDGs and child survival over the project’s first 3 years and compare these changes to local trends”. The authors examined child mortality rates across 9 Millennium Villages and concluded: The average annual rate of reduction of mortality in children younger than 5 years of age was three-times faster in Millennium Village sites than in the most recent 10-year national rural trends (7·8% vs 2·6%). When this PEER REVIEWED article was published online, Bump et al. (2012) challenged the calculation of both of these figures and the resulting interpretation that the MVP had a large impact on child mortality. One of the authors of this criticism, Gabriel Demombynes, explained the errors in this post. First, Pronyk et al. (2012) annualized (aka, divided) the 21.7% cumulative decline in mortality by 3 for the number of years the intervention was active rather than by 4, the correct time window for this retrospective assessment of mortality. This correction reduces the annual rate of decline among Millennium Villages from a claim of 7.8% to an actual 5.4%. Second, Pronyk et al. (2012) compared their estimated annual decline after 2006 (7.8%) to the results of national surveys in MVP countries from 2001-10. A more appropriate comparison using all of the available post-2006 data suggests an average annual rate of decline of 6.4% across the 9 MVP countries. Bump et al. (2012) conclude: The above observations imply that a key finding of the paper—that child mortality fell at the treatment sites at triple the nationwide rural background rate—is incorrect. Child mortality fell at 5·9% per year at the sites versus 6·4% per year on average across all areas of the countries in question (probably more in rural areas alone) according to the available data that most closely match the project period. This difference is not significant. Pronyk (2012) published a correction that was noted by Retraction Watch. This is an example of post publication peer-review. 4.2.1 GETTING PAST THE GATEKEEPERS Let’s say you just completed what you think is a fascinating new study that upends years of conventional thinking on your research question. You could issue a press release and tell the world, but most scientists would reserve judgement or consider your results preliminary pending peer review. They would expect you to write a manuscript detailing your research design, data, methods, and results, and then submit this manuscript to an academic or scholarly journal. Typically, your submission would be screened by the journal’s editor, a role often filled by a senior scientist in your field. If the editor thinks that your paper is free of obvious fatal flaws and will be of interest the journal’s readership, then the editor might assign it to an associate editor with some expertise on your topic to manage the peer review process. 4.2.2 WHO IS A PEER? The associate editor will then attempt to find 3 or more scholars in your field—your peers—to review the paper and comment on its merits. Some journals give you the option to recommend reviewers who might be a good fit and to request that certain colleagues are not considered. The editorial team does not need to respect your wishes, but finding appropriate reviewers is a challenge and you can help by suggesting scholars who are qualified to evaluate your work. So a “peer” can be: someone at your level, more junior, or more senior someone who shares the same conceptual framework regarding your topic of study, or someone who takes a different view entirely someone who works on a parallel topic, or someone who is a direct “competitor” someone who is a topic expert, or, when it’s hard to find the right person, someone who does not have much background at all someone who is a technical expert on your methods, or someone who does not know the first thing about your chosen analytical approach You’ll probably never know. Most journals use a blind review process by which you don’t know who accepts the editor’s request to review your paper, and the reviewers are not informed of your identity. At least that’s the idea. Sometimes reviewers give hints about their identity by recommending that you cite a lot of their own work. And sometimes it’s easy to determine your identity as the author because your current work builds on your previous studies or you’ve already presented the work at scientific conferences. 4.2.3 WHAT HAPPENS DURING THE REVIEW PROCESS? Once your paper arrives on a reviewer’s desk, he or she will take a few weeks (or months!) to recommend that your paper be rejected or accepted with no, minor, or major revisions. Some reviewers enumerate your perceived flaws in painstaking detail. Others give high-level comments that might be too vague to be helpful to you or the editor. The editorial team reviews these reviews, and it’s up to them to make a decision. Most academics are happy to get a “revise and resubmit” letter (aka, “R&amp;R”). The editor will usually give some indication that a revised paper would have a good chance of publication, but it’s not a guarantee. Sometimes the revised paper will go back out for further review, but the editor can also make the decision to accept the revision without additional input. The editor has a tough task because it’s often the case that reviewers take different positions on your submission. As Smith (2006) suggested, these recommendations can be direct opposites: Reviewer A: `I found this paper an extremely muddled paper with a large number of deficits’ Reviewer B: `It is written in a clear style and would be understood by any reader’. So peer review is just like course evaluations! 4.2.4 WHAT DOES NOT HAPPEN? A critical thing to note, however, is that reviewers almost never have access to your data or analysis code. They base their decisions on what you said you did (your methods), what you said you found (your results), and what you said it all means (your discussion). You have to describe your data sources, but no one is checking your work. So even if reviewers find possible flaws in your logic, analysis mistakes and fraud go largely unchecked.19 This lack of verification is why it is wrong for people to conclude that published in a peer-reviewed journal means correct. Journalists can harbor this belief, and defensive authors sometimes promote it when challenged on their study’s findings. If it were true that published==correct, then we’d have no need for corrigendum and retractions—and the website Retraction Watch would be empty. 4.2.5 PEER REVIEW OF FUNDING PROPOSALS Funding agencies like the NIH also use a peer review process to make funding decisions. When you send your grant to an institute at the NIH, a Scientific Review Officer (SRO) checks the proposal for completeness and assigns it to several peer reviewers serving on a Scientific Review Group. The reviewers write up their critiques and assign a score from 1 (exceptional) to 9 (poor). If your preliminary score is too low, the full committee may not discuss your proposal and it’s the end of the road (unless you decide to improve and resubmit). If your proposal is discussed, the committee assigns a final score (multiplied by 10 to be on a scale of 10-90). After the meeting, you’ll receive a summary statement with comments, the overall impact score, and the percentile score. If your proposal is above the payline—a percentile score that most institutes set based on the available budget—then your proposal will likely be funded. The overall success rate in 2015 was 18.3%. 4.3 How to be a Good Consumer of Research Here’s a nice framework for how to be a good peer reviewer (aka, referee) that is a good starting point for thinking about how to be a good consumer of research more generally. In this guide, Leek describes a scientific paper as consisting of four parts.20 I take the liberty of collapsing methodologies and data, as well as adding the Introduction. An introduction that frames the research question A set of methodologies and a description of data A set of results A set of claims Leek offers a helpful recommendation about how to approach a new paper: Your prior belief about [#2-3] above should start with the assumption that the scientists in question are reasonable people who made efforts to be correct, thorough, transparent, and not exaggerate. This book focuses mainly on how to read and write sections 1 and 2. I think you’ll gain new insight into how to evaluate research results and claims if you make it to the end, but it’s probably safe to say that you’ll need more background in analysis to feel confident in your ability to critique findings and plan your own analysis. 4.3.1 INTRODUCTION SECTION A good Introduction will explain the aim of the paper and put the research question in context.21 In public health and medicine, this section will often be very short compared to what you’ll find in other disciplines like economics. As a reader, you want to focus on understanding the research question. If you are familiar with the research area, you might also read the Introduction with a critical eye toward the literature reviewed. Did the authors miss any key references that could signal that they are unaware of developments in the field?22 4.3.2 METHOD SECTION A good Method section will provide enough information to let the reader attempt to replicate the findings in a new study. Journal space constraints make this challenging, so you’ll often find that authors post supplemental materials online that give additional details.23 Even with supplemental materials, however, it would be common to need to contact the author for additional details and materials if you really wanted to attempt a replication. The organization of the Method section will vary by discipline, but you should expect to find some information about the research design, subjects, materials or measures, data sources and procedures, and analysis strategy. The Equator Network, which awkwardly stands for “Enhancing the QUAlity and Transparency Of health Research”, is a good resource for understanding modern reporting standards. If you are preparing your own manuscript, most journals will expect you to include all of the information outlined in the checklist that’s relevant for your research design. If you are reviewing an article, you can do your part to promote comprehensive reporting by referring authors to these checklists. Include the completed checklist as an appendix with your article submission to head off reviewers who will complain about missing information that you definitely included on page 5, line 20 thank you very much. Is the research design well-suited to answer the research question? This book will introduce you to common research designs in global health. What you should know at this point in your reading is that there are many different designs that could potentially answer most research questions, but not all designs are created equal. A graphic like the one you see in Figure 4.1 is commonly used in the EBM literature to convey this point. The meta-analyses and systematic reviews that you read about in Chapter 3 are ‘studies of studies’ and they sit atop the evidence hierarchy. They enjoy this status because they synthesize the best available evidence. No one study is the final word on a research question, so it should make sense that a meta-analysis that pools together results and accounts for variable study quality could potentially give you a better answer than any one study alone. Figure 4.1: Levels of evidence However, the Cochrane Handbook for Systematic Reviews (2011) cautions us to pay attention to design features (e.g., how participants were selected) rather than labels (e.g., cohort study) because such labels are broad categories. Therefore, don’t rely too much on this hierarchy. These rankings reflect ideals. It’s possible to have a poorly designed or implemented RCT. The evidence from such a flawed study will not necessarily be better than the evidence from a non-randomized study just because it carries the label “randomized”. Is there a risk of bias and confounding? Some study designs are better than others (at least in theory) because of their ability to address potential bias when conducted properly. As we discussed in Chapter 2, the goal of scientific research is inference and we must live with some error and uncertainty. As a consumer of research, you have to accept this as fact and assess the extent to which a study’s design and methods might lead us away from the “truth”. Error comes in two flavors: random and systematic. Random error adds noise (aka, variability) to your data, but it does not affect the average. For instance, I might step on a scale and see that I weigh 185.12. I step off and back on, and this time I weigh 185.13.24 This is random error that results from the limitations of my scale. If I keep taking measurements, this random error will balance out. Random means that the readings won’t be systematically too high or too low. You’ve probably already guessed that systematic error is not random. Systematic error is also known as bias and represents a deviation from the “truth”. Let’s imagine that my scale is broken and I don’t really weigh 185. I weigh 200. I can worry about the imprecise measurements of 185.12 and 185.13 all day, but I’d be missing the bigger problem that my scale is systematically reading the wrong weight. I can keep taking measurements over and over, but my scale is just wrong. If my goal is 186, I would come to the wrong conclusion that I can stop dieting! You can estimate random error (as we’ll discuss in Chapter 9), but you typically don’t know the extent to which bias affects your study results. For this reason, we often frame this as a “risk of bias”. In a non-randomized design, the biggest risk of bias comes from potential selection bias (Higgins and Green 2011). Selection bias can take different forms. In the context of intervention research, selection bias represents pre-treatment (aka, baseline) differences between study groups. For instance, Webster et al. (2003) conducted a case-control study—a non-randomized, or observational study design that we’ll discuss more in Chapter 13—in Eastern Afghanistan to study the efficacy of bednets as a tool for preventing malaria. Patients who presented at the study clinic with a fever were tested for malaria. Those who tested positive were classified as “cases”, and the rest were classified as “controls”. The researchers asked cases and controls about their bednet use, education, income, and several other characteristics. Then they compared bednet users and non-users on their odds of malaria (i.e., being classified as cases). Webster et al. (2003) wanted to look at potential selection effects with this particular research design, so they also examined patients’ use of chloroquine prior to attending the clinic. If a patient was classified as a control (negative blood film) but tested positive for chloroquine, this would indicate that the patient received treatment for malaria prior to arriving at the clinic, meaning they really should have been classified as a case. To determine if this misclassification of cases as controls could introduce selection bias, the authors looked at chloroquine use in bednet users and non-users. They found that the use of chloroquine prior to clinic testing was LESS common among patients who reported using bednets compared to non-users. If chloroquine use was less common among bednet users, it would underestimate the estimated effect of bednets. Consider the following example. Panels A and B show cases (those who tested positive for malaria) and controls by their reported bednet usage. In Panel A, there are 4 patients who are misclassified as controls, meaning that they tested negative for malaria but only because they treated themselves with chloroquine prior to the test. You can see that chloroquine use is less common among net users. Still in Panel A, we see that the odds of malaria (cases) among bednet users is 12/30 and the odds of malaria among non-users is 12/18. This is an odds ratio of 0.60, suggesting that bednets protect against malaria (a value of 1 would indicate no effect). However, Panel B shows that this effect might be an underestimate. If we move the misclassified control patients to the case group where they belong, the odds change. Now the odds of malaria among bednet users is 13/30 and the odds among non-users is 15/18. This is an odds ratio of 0.52, suggesting an even greater protective effect. In Panel A, the effect was biased toward the null, meaning that the effect looked smaller than it probably is in reality. This bias results in confounding, and we would call chloroquine use a confounding variable. Confounding variables are correlated with both the “treatment” (i.e., bednet use) and the outcome (i.e., malaria). Exploring selection bias As we’ll explore later, an experimental design typically overcomes the risk of bias and confounding through random assignment. If the sample size is large enough, potential confounding variables like chloroquine use from the example above should be equally likely likely for all groups. The key word here is “typically”. Things can still go wrong in an experimental design that result in a risk of bias. For this reason, every Cochrane systematic review assess several types of known risks of bias in RCTs (Higgins and Green 2011): selection bias performance bias detection bias attrition bias reporting bias We’ll discuss these sources of bias more in Chapter 11, but the takeaway should be that every study has a potential for bias and, as a reviewer or general research consumer, you should assess the risks of bias that might challenge the validity of the results. As you’ll see later, this type of validity—are the study results “correct”?—is typically referred to as internal validity (Higgins and Green 2011). At the end of this chapter you’ll read about another dimension of validity called external validity. PDF slide deck Who (or what) was the subject of study and how were these subjects recruited and/or selected? Studies of human subjects typically have a subsection of the Method section that describe participant selection and recruitment. What made someone eligible or ineligible to participate? Who was excluded, intentionally or not? These details help to define the population of interest and will inform the study’s generalizability, a concept we’ll discuss shortly. Once eligible participants were identified, how were they selected and recruited? Was this process random, or did the researchers invite who was available? As you’ll learn in Chapter 9, the method of sampling has implications for what inferences are possible about the population. What materials and/or measures were used in the course of the study? Almost every study uses some type of materials or measures. Diagnostic studies, for instance, evaluate a diagnostic test or a piece of hardware that analyzes the test samples. Environmental studies might use sophisticated instruments to take atmospheric measurements. Expect studies like these to provide specific details about the materials and equipment. Study variables also need to be precisely defined. For instance, hyperparasitemia describes a condition of many malaria parasites in the blood. But what constitutes “many”? The WHO defines it as “a parasite density &gt; 4% (~200,000/µL)” (WHO 2015b). Does the study use this definition? Another one? In studies measuring social or psychological constructs such as anxiety, you’d expect to read about how this thing called “anxiety” is defined and measured. Is anxiety diagnosed by a psychiatrist (if so what is the basis for this diagnosis?) or is anxiety inferred from a participant’s self-reported symptoms on a checklist or screening instrument (if so, what are the questions and how is the instrument scored?)? We’ll dive into measurement issues in Chapter 8. How was the study conducted and how was the data collected? This part of a Method section should describe what happened after participants were recruited and enrolled. What happened first, second, third? If the study is observational, the procedures might be limited to data collection. Who collected the data, and how were they trained? Where were the data collected? For intervention studies, the procedures will describe how participants were randomized to study arms and what happened (or did not happen) in each arm. Were the participants, data collectors, and/or patients blind to the treatment assignment? How was the data analyzed? If the study uses a hypothesis-testing framework (and not all do), then you’ll find details about study hypotheses in the Introduction or Method section, depending on the journal. The Method section should also detail how the analysis will be carried out. For instance, if you are reading an intervention study, how was the effect size be estimated? Ordinary least squares regression? Logistic regression? The list goes on and on. When you are preparing your own manuscript, remember that the Method section is where you should define variables and specify your analysis. Your Results section should just get to the business of reporting the findings. There’s no need to re-explain your analysis. Was the study pre-registered and approved by an ethics board? The US Federal Policy for the Protection of Human Subjects (aka, the “Common Rule”) defines research as “a systematic investigation, including research development, testing and evaluation, designed to develop or contribute to generalizable knowledge…” If the research involves human subjects, it must be reviewed and approved by an Institutional Review Board before any subjects can be enrolled. Most studies fall under IRB oversight, but some might qualify as exempt. Increasingly researchers are taking the additional step of registering a study protocol prior to the study launch in a clearinghouse like https://clinicaltrials.gov/. This is a requirement for investigations of drugs that are regulated by the FDA, and now it’s an expectation of many journals.25 Pre-registration does not ensure trustworthy results, but the practice is a welcome increase in research transparency. If the analysis described in an article deviates from the planned analysis, you would expect the authors to provide a compelling justification. Studies often measure a number of outcomes (sometimes in a number of different ways), and it’s easy to cherry-pick results and find something to present. Sometimes authors will actually deviate from the pre-registered protocol and present different results when the pre-registered plan does not work out. This is called outcome switching. Some medical journals don’t seem to care, but the COMPare Trials Project thinks they should. Figure 4.2: Is outcome switching a problem in medical trials?; Source: http://compare-trials.org/. 4.3.3 RESULTS SECTION Can each finding be linked to data and procedures presented in the Methods Every finding in the Results section should be linked to a methodology and source of data documented in the Method section. Articles in medical journals are some of the shortest, so you might need to download supplemental materials posted online to get a clearer sense of what the authors did and found. Remember, a plot twist can be a useful literary device in a work of fiction. This advice does not extend to journal articles. Is the analysis correct? Without access to the data and any analysis code—still the norm for most publications—you can’t independently verify the results. Even if you did have access, some analyses are so complex that only people with extensive training feel qualified to question the accuracy of the results. 4.3.4 DISCUSSION SECTION Is each claim linked to a finding presented in the Results? Each claim (e.g., the world is flat) should be supported by results that are reported in the paper (e.g., summary of altitude data). If you don’t find a link between a claim in the Discussion section with a finding in the Results section, you should begin to wonder if the author is “going beyond the data”. For instance, if I present results on the efficacy of a new treatment for malaria but do not present any data on cost, then it would be inappropriate for me to claim that the treatment is “cost-effective”. It’s legitimate to speculate a bit in the Discussion section based on documented findings, but authors should be careful to label all speculation as such—and these thought exercises should never find their way into the article’s Abstract. Is each claim justified? Once you establish that there is a link between a claim and a set of results, you want to make sure that the claim represents a correct interpretation of these findings. You also want to make sure that the authors do not “go beyond the data” by making conclusions that are not supported by the analysis. For instance, finding weak or mixed evidence that a new program works and recommending massive scale-up. Or claiming that a program is cost-effective but not presenting data on actual costs. Are the claims generalizable? Most [studies] are highly localized and paticularistic…Yet readers of [your study’s] results are rarely concerned with what happened in that particular, past, local study. Rather, they usually aim to learn either about theoretical constructs of interest or about a larger policy. That’s Shadish et al. (2003) writing about the importance of generalizability of research findings and claims. When a study is so highly localized that the results are unlikely to generalize to new people and places, we’d say that the study has low external validity. One approach to promoting generalizability is to use formal probability sampling. We’ll cover this more in Chapter 9, but randomly sampling participants from the population of interest is one way to increase the external validity of a study. For instance, Wanzira et al. (2016) analyzed data from the 2014 Uganda Malaria Indicator Survey, a large national survey, and found that women who knew that sulfadoxine/pyrimethamine (SP) is a medication used to prevent malaria during pregnancy had greater odds of taking at least two doses as compared to women who did not have this knowledge. Since the UMIS is nationally representative, we could assume that the results could apply to Ugandan women who did not participate in the study. Would the results generalize to women in Tanzania? Yes, one could make an argument that they would. Would the results generalize to women in France? No, probably not. For one, malaria is not an issue there. Are the claims put in context? A good Discussion section will put the study’s findings in context by suggesting how the study adds to the literature. Do the results replicate or support other work? Or do the findings run contrary to other published studies? What are the limitations? No study is perfect, so there’s no need to pretend that yours is any different. In addition to knowing how the results fit into the bigger research landscape, it’s also important for readers to understand potential limitations of your design and method. Additional Resources Critical appraisal worksheets from the Centre for Evidence-Based Medicine BMJ Series on “How to Read a Paper” Critical appraisal resources from Duke Medicine Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["causeeffect.html", "5 Cause and Effect 5.1 Fundamental Challenge of Causal Inference 5.2 Threats to Internal Validity 5.3 Research Designs to Estimate Causal Impact Share Feedback", " 5 Cause and Effect Do bednets prevent malaria? Do vouchers increase access to treatment? Do cash transfers improve mental health? What these research questions share in common is a focus on causal impact—a difference in outcomes that can be attributed to a treatment, intervention, policy, or exposure. In many ways, as an applied field global health is the study of causal impact. 5.1 Fundamental Challenge of Causal Inference PDF slide deck In an ideal research world we could answer the question, “Do bednets prevent malaria?”, by cloning you and simultaneously giving a bednet to you but not your clone. This would help us to understand what really happens in the absence of the intervention because the only difference between you and your clone would be that one of you received the intervention.26 Of course we don’t have clones, and we can’t simultaneously give you a bednet and not give you a bednet. We only get to observe what happens to you, not a clone of you who did not receive the intervention. So we have to ask, hypothetically, what would have happened if you had not been given a bednet. This hypothetical situation—what would have happened in the absence of the intervention—is referred to as the counterfactual (or “potential outcome” in the language of the Neyman–Rubin causal model). Not being able to observe the counterfactual directly is the so called “fundamental challenge of causal inference”. So you could also say that this is the primary reason we need books about research designs. Much of what follows in this book deals with strategies for causal inference in the absence of a true counterfactual. 5.1.1 UNDERSTANDING CAUSAL RELATIONSHIPS Humans like you and me have a pretty decent understanding of cause and effect—hand touch fire, fire hot, fire burn hand. Philosopher humans, on the other (unburnt) hand, have spent centuries convincing us that causality is actually much more complicated than it might seem on the surface. They did a good job because causal inference is a vibrant field of study today, and researchers continue to develop new techniques for drawing causal inferences from experimental and non-experimental data. Let’s review some of the basics. Causes In his book Causal Inference in Statistics, computer scientist Judea Pearl (2016) provides a simple definition of causes: “A variable X is a cause of a variable Y if Y in any way relies on X for its value”. The phrase “in any way” is a reminder that most of the causal relationships we investigate in global health are not deterministic and effects can have more than one cause. Think about it this way: you give an experimental treatment to 100 people suffering from a disease and only 60 get better. If the causal relationship between the drug and disease state were deterministic, all 100 patients would have recovered. This is not what happened, however. The causal relationship only increased the probability that the effect would occur. Effects Causal impact is the difference in counterfactual outcomes (aka, “potential outcomes”) caused by some exposure, program, intervention, or policy. Sounds simple enough, but we’re back to our fundamental problem: we can only observe one counterfactual outcome for an individual; we don’t get to observe someone in two states simultaneously, e.g., treatment and control. This means we can’t observe an effect of the program on an individual. We have to turn instead to groups of individuals. The best we can do is hope to infer the counterfactual by comparing some people who get some treatment to other people who do not.27 Most often we think about comparing two different sets of people who who exist in either a treatment or an intervention group, but the logic also extends to (a) more than two groups (aka, study arms) and (b) just one group of people observed at different time points. “But I thought you said an individual like me can’t exist in two states at once!” That’s correct. We can only observe someone in one state (e.g., treatment or control), but we could chose to compare YOU before you receive a treatment to YOU after you receive a treatment. This is a “pre-post” or “before/after” comparison. We’ll talk more about specific designs soon. The most common estimate of causal impact is the average treatment effect (ATE). We can’t observe an effect of X on Y for any specific individual (who can only exist in one state at a time), but we can determine if X causes Y on average. This is possible because the average difference in potential outcomes (which we can’t observe) is equal to the difference of averages. The following graphic might help.28 Figure 5.1: Average causal effects can be estimated even though individual effects cannot be observed. Panel A shows hypothetical results when all subjects are assigned to treatment Y(1) or control Y(0). There are two data points for each person corresponding to their hypothetical potential outcomes (remember that in reality we can only observe one point per person since it’s not possible to be in two states at once). Both sets of points have an average value, depicted by the dashed lines that you see in Panel A and Panel C. In the middle Panel B, the differences between each pair of outcomes in A are plotted as individual effect sizes (e.g., 1.0-(-1.5)=2.5 for person 6). The dashed green line depicts the average causal effect. Notice that this average of individual differences in B is equal to the difference in averages in Panel C. The point is that we cannot observe effects on individuals since we cannot measure both potential outcomes for any one person; however, we can estimate the average causal effect by comparing the average value from a group of people who receive the intervention to the average value from a group of people who do not. Causal relationships OK, so we’ve established that it’s possible to estimate the average difference between two groups, but can we claim that this difference is a valid estimate of the causal impact of X on Y? In other words, how do we know that X and Y causally related? Shadish et al. (2003) point to three characteristics of causal relationships that we should be on the lookout for: The cause is related (aka, associated) to the effect. The cause comes before the effect. There are no plausible alternative explanations for the effect aside from the cause. Condition #1 is easy to establish. Is X correlated with Y? In fact it’s so easy to establish that someone came up with the maxim, “correlation does not prove causation”, to remind us that the burden of proof is greater than the output of correlate x y or cor(x, y), or whatever command your favorite statistical software package would have you run. But it’s a start. Condition #2 is a bit harder to demonstrate conclusively because X and Y might be correlated, but maybe the causal relationship runs in the opposite direction—maybe Y causes X. Correlations don’t tell us which comes first, X or Y. Let’s take malaria and poverty as an example. Jeffrey Sachs and Pia Malaney (2002) published a paper in Nature in which they wrote: As a general rule of thumb, where malaria prospers most, human societies have prospered least…This correlation can, of course, be explained in several possible ways. Poverty may promote malaria transmission; malaria may cause poverty by impeding economic growth; or causality may run in both directions. Condition #3 is the trickiest of all: ruling out plausible alternative explanations. As Sachs and Malaney note, the literature on poverty and malaria has not found a way to do so conclusively. They write that it’s “possible that the correlation [between malaria and poverty] is at least partly spurious, with the tropical climate causing poverty for reasons unrelated to malaria.” The authors are proposing that climate is a potential cause of both poverty and malaria. If true, that would make climate a confounding (aka, lurking) variable that accounts for the observed relationship between poverty and malaria. 5.2 Threats to Internal Validity The possibility of plausible alternative explanations is what keeps researchers up at night, particularly non-experimentalists. They are always on the lookout for threats to making valid causal inferences—aka, threats to internal validity. The “randomistas”, on the other hand, tuck themselves in early after a glass of warm milk knowing that random assignment will generally make plausible alternative explanations implausible. To be sure, the randomistas will have bias-filled nightmares on occasion when they learn that an experiment did not quite go as planned, but they are generally heavy sleepers. You’ll recall that internal validity is Campbell’s (1957) notion about whether an observed association between X and Y represents a causal relationship. If X comes before Y, and if there are no other plausible explanations for the covariation between X and Y then causal inference about X and Y is valid. Threats to causal inference are threats to internal validity. Shadish, Cook, and Campbell (2003) outlined nine primary reasons why it might not be valid to assume a relationship between X and Y is causal. Table 5.1: Threats to internal validity. Source: Shadish et al. (2002), http://amzn.to/2cBaAM1. Threats Definitions Ambiguous temporal precedence Lack of clarity about which variable occurred first may yield confusion about which variable is the cause and which is the effect. Selection Systematic differences over conditions in respondent characteristics that could also cause the observed effect. History Events occurring concurrently with treatment could cause the observed effect. Maturation Naturally occurring changes over time could be confused with a treatment effect Regression When units are selected for their extreme scores, they will often have less extreme scores on other variables, an occurrence that can be confused with a treatment effect. Attrition Loss of respondents to treatment or to measurement can produce artifactual effects if that loss is systematically correlated with conditions. Testing Exposure to a test can affect scores on subsequent exposures to that test, an occurrence that can be confused with a treatment effect. Instrumentation The nature of a measure may change over time or conditions in a way that could be confused with a treatment effect. Additive and interactive effects The impact of a threat can be added to that of another threat or may depend on the level of another threat. 5.2.1 AMBIGUOUS TEMPORAL PRECEDENCE Correlational studies can establish that X and Y are related, but often it’s not clear that X occurred before Y. Uncertainty about which way a causal effect might flow is referred to as ambiguous temporal precedence—or simply the chicken and egg problem. Sometimes the direction is clear because it’s not possible for Y to cause X. For instance, hot weather (X) might drive ice cream sales (Y), but ice cream sales (Y) cannot cause the temperature to rise (X). Most relationships we care about in global health are not so clear, however. Take bednet use and education as an example. Does bednet use prevent malaria and allow for greater educational attainment? Or does greater education lead to a better understanding and appreciation of the importance of preventive behaviors like bednet use?29 5.2.2 SELECTION As we discussed earlier, the fundamental challenge of causal inference is that we cannot observe the counterfactual directly. So often we look to compare a group of people who were exposed to the potential cause to a group of people who were not exposed. No matter how hard we try to make sure that these two groups of people are equivalent before the treatment occurs, there can be observable and unobservable ways in which these groups differ. These differences represent selection bias, a threat to internal validity. For instance, Bradley et al. (1986) compared parasite and spleen rates among bednet users and non-users in The Gambia and concluded that bednets had a “strong protective effect” against malaria; however, the authors also observed that bednet use and malaria prevalence were also associated with ethnic group and place of residence. This makes ethnic group and place of residence confounding variables—plausible alternative explanations for the relationship between bednet use and malaria. Identifying selection bias and trying to account for it in the analysis is like squashing an ant on your countertop. You got that one, good for you, but there are probably many more hiding in the walls. You just can’t see them. It’s the same with selection threats. You might measure some of them, but many confounding variables will probably go unobserved. The only way to be certain that you are free of such threats is to randomly assign people to conditions. 5.2.3 HISTORY History threats pick up where selection threats leave off. Whereas selection threats are reasons that the groups might differ before the treatment occurs, history threats occur between the start of the treatment and the posttest observation. Before/after studies (aka, pre-post studies) are particularly susceptible to history threats. In these designs researchers assess the same group of people before and after some intervention. There is no separate control or comparison group. The assumed counterfactual for what would have happened in the absence of the intervention is simply the pre-intervention observation of the group. Okabayashi et al. (2006) is a good example. In this study, the researchers conducted a baseline survey and then began a school-based malaria control program. Nine months later, they conducted a post-program survey with the same principals, teachers, and students. On the basis of the pre-post differences they observed, the authors concluded the educational program had a positive impact on preventive behaviors. For instance, student-reported use of bednets (“always”) increased from 81.8% before the program to 86.5% after the program. It’s possible that the program changed behavior, but without evidence to the contrary, it’s also possible that something else was responsible for the change. Maybe another program was active at the same time. Maybe there was a marketing campaign for a new type of bednet just hitting the market. Maybe the posttest occurred during the rainy season when people know the risk of malaria is greater. These are all examples of possible history threats that could invalidate causal inference about the impact of the program on behavior change. 5.2.4 MATURATION Single group designs like Okabayashi et al. (2006) are also subject to maturation threats. The basic issue is that people, things, and places change over time, even in the absence of any treatment. This is easy to imagine if you picture the learning that happens in one year of school for young kids. Comparing kids at the end of the year to their younger selves a year earlier and making a causal inference about some program or intervention is problematic because kids gain new cognitive skills as they age. Maybe the change you observe is due to a specific program or intervention, or maybe it is just the passage of time. Without a comparison group of similar aged kids it can be hard to know the difference. 5.2.5 REGRESSION ARTIFACTS This threat occurs most often when people are selected for a study because they have very high or very low scores on some outcome. It’s often the case that scores will be less extreme at retest, independent of any intervention. This statistical phenomenon is called regression to the mean. It happens because of measurement error and imperfect correlation. 5.2.6 ATTRITION Attrition happens when study participants do not participate in outcome assessments. When attrition is uneven between study groups, it can be described as systematic attrition. Just like selection bias makes groups unequal at the beginning of a study, attrition bias makes groups unequal at the end of the study for reasons other than the treatment under investigation. For instance, let’s say that researchers recruit depressed patients to take part in an RCT of a new psychotherapy that is delivered over the course of 10 weekly sessions. If the most depressed patients in the treatment group drop out because the schedule is too demanding, then the analysis would compare the control group (with the most depressed patients still enrolled) to a treatment group that is missing the most depressed patients.30 It would appear as if the treatment group got better on average, but part or all of the observed treatment effect would be due to attrition of the most depressed patients from the treatment group, not because of the treatment. 5.2.7 TESTING Repeated administrations of the same test can have an effect on test scores, independent of the program that the test is designed to evaluate. For instance, practice can lead to better performance on cognitive assessments, and this improved performance can be mistaken as a treatment effect if there is not a comparison group. Testing threats decrease as the interval between administrations increases. 5.2.8 INSTRUMENTATION Testing threats describe changes to how participants perform on tests over time due to repeated administrations. When the tests themselves change over time we call this an instrumentation threat. For instance, if a study uses different microscopes or changes measurement techniques for the posttest assessment, it’s possible that differences in blood smear results could be incorrectly attributed to an intervention. 5.2.9 ADDITIVE AND INTERACTIVE EFFECTS Unfortunately, a study can be subject to more than one of these threats to internal validity. It’s possible for threats to work in opposite directions, or to interact and make matters a lot worse. For instance, if Okabayashi et al. (2006) had decided to compare students who received the program to students from another part of the country who did not receive the program, they might have observed a selection x history threat. The two groups of students might have been different to begin with (selection) and might have had different experiences over the study period unrelated to their treatment or non-treatment status (history). 5.3 Research Designs to Estimate Causal Impact Figure 5.2: Research design choose your own adventure. PDF download, https://drive.google.com/open?id=0Bxn_jkXZ1lxuWkhFcTUzdWVkZ0E 5.3.1 EXPERIMENTAL DESIGNS If given the choice, many (if not most) researchers would choose an experimental design to estimate causal impact. Experiments are subject to bias when things don’t go as planned (e.g., systematic attrition), but a good experiment is subject to fewer threats to internal validity compared to every other design for two main reasons: The cause always comes before the effect in an experiment (and quasi-experiment) because the treatment is “manipulated”; some people get the treatment but others don’t. After the treatment is administered to some people, outcomes are observed. Cause before effect. Random assignment makes plausible alternative explanations implausible. This is the big one. Whereas other designs require stronger assumptions about selection threats, experiments dismiss them by distributing observable and unobservable differences approximately equally across study arms. Figure 5.3: Basic experimental design Example An important global health policy question that has been studied using experimental and quasi-experimental methods is the impact of user fees on the adoption of health goods, such as bed nets. Advocates of fees argue that free distribution is not sustainable and leads to waste when people who don’t need or want the goods are recipients. There is also an argument that people only value what they pay for, so removing fees will make people less likely to use goods like bed nets. The flip side is that the provision of some health goods, in economics-speak, creates “positive externalities” and should therefore be financed with public dollars. What this means is that some interventions have spillover effects whereby people who are not treated still experience some indirect effect. A good example of a spillover effect is vaccines and the resulting herd immunity. Hawley et al. (2003) showed a similar protective effect of ITN use on child mortality and other malaria-related outcomes among households without ITNs that were located within 300 meters of households with ITNs. So we know that there is evidence that ITNs have direct (Phillips-Howard et al. 2003) and indirect benefits. The research problem is then how to increase coverage and use of nets. Is free distribution the best strategy, or should users have to spend something to get a bed net that might retail for a price that is out of reach for many poor households? In other words, should ITNs be free or subsidized? Cohen and Dupas (2010) used an experimental design to study this question in Kenya where malaria is the leading cause31 of morbidity and mortality. The authors randomly assigned 20 prenatal clinics in an endemic region to 1 of 5 groups: a control group that did not distribute ITNs, a free distribution group, a group that charged 10 Ksh per ITN (97.5% subsidy), a group that charged 20 Ksh (95% subsidy), and a group that charged 40 Ksh or about $0.60 USD (90% subsidy). When units like clinics, schools, and villages are randomized, we refer to the design as a cluster-randomized trial, or CRT. The authors followed up a subset of pregnant women over time and found that those who paid a subsidized price were no more likely to use the bed nets than women who received one for free. They also found that the increase in price from $0 to $0.60 USD reduced demand for ITNs by 60%. This implies that the cost-sharing model of having women pay something for ITNs will reduce coverage. This is bad for the women who forgo a net purchase because of the direct prevention effects of ITNs, but we know from Hawley et al.’s work that it’s also bad for the community since ITNs have spillover effects. Cohen and Dupas conclude that free distribution would ultimately save more child lives. 5.3.2 QUASI-EXPERIMENTAL DESIGNS As great as experiments are for mimicking the counterfactual, it’s not always logistically possible, politically feasible, or ethically justified to run an RCT. Most often, researchers have to infer causal inference from non-experimental data. How this is done in practice is shaped in part by disciplinary traditions. For instance, psychologists trained in the tradition of Campbell tend to focus on design choices you can make before a study is launched to improve causal inference by ruling out alternative explanations (Shadish, Cook, and Campbell 2003). This is the idea of the primacy of control by design—try to prevent confounding or at least investigate the plausibility of alternative explanations by adding design elements like more pre-test observations and comparison groups. Economists have a similar preference for strong designs, but their approach to causal inference tends to focus more on the analysis that comes after data collection. Whereas psychologists might ask about threats to internal validity, economists are likely to ask “what’s your identification strategy?”. Econometricians Angrist and Krueger (1999) defined identification strategies as “the combination of a clearly labeled source of identifying variation in a causal variable and the use of a particular econometric technique to exploit this information.” For instance, economists spend a lot of time thinking about the returns on schooling. The most common identification strategy to estimate the impact of schooling (the proposed causal variable) uses regression to control for potential confounds. In addition to regression, the econometrics (or ’metrics, if you are cool) toolkit for non-experimental data also includes instrumental variables, regression discontinuity, and differences-in-differences (Angrist and Pischke 2015). I’ll add interrupted time series to the list. Psychologists (and others) would label these quasi-experimental designs because they involve some manipulable cause that occurs before an effect is measured but lack random assignment. Figure 5.4: Common quasi-experimental designs Example Agha et al. (2007) used a quasi-experimental design to estimate the impact of a social marketing intervention on ownership and use of ITNs in rural Zambia. Nets that commonly sold for USD $27 were subsidized and sold for $2.50 at public health clinics. Neighborhood health committees were established and 600 volunteer “promoters” were trained to teach residents about malaria and encourage them to purchase the nets. To estimate the impact of the intervention, the authors analyzed data from post-intervention surveys in three intervention and two comparison districts. This study design was quasi-experimental because the districts were not randomized to the intervention or control arms. Figure 5.5: Source: Agha et al. (2007), http://bit.ly/1MkO5a0 Agha and colleagues reported that ITN ownership and use was higher in intervention districts according to the post-intervention data, but were careful to avoid going ‘beyond the data’ to claim evidence of a causal relationship. There are several design limitations to consider here, and you will learn more about how to spot these issues as we go. Briefly, we can note that (i) the authors did not randomize districts to study arms and (ii) no baseline (a.k.a. pre-treatment) data was collected. Experimental studies benefit from, but do not require, baseline (or pre-intervention) data because randomization usually ensures that the treatment and comparison groups are similar at the start—if enough units are randomized. But a non-randomized study like this leaves itself open to criticism without baseline data to show that the intervention and comparison districts were similar before the intervention was introduced. The results suggest that they were different after the intervention period, but we can’t be sure this was caused by the intervention itself. Given the limitations, how should we view the results? If this was one of the first studies on the topic, we would view it as a starting point that would encourage more rigorous investigations. As part of a larger body of evidence, however, it would probably be passed over in systematic reviews and meta-analyses—studies of studies—because of the limitations of the design for causal inference. 5.3.3 OBSERVATIONAL DESIGNS Epidemiologists are typically associated with observational designs such as cross-sectional surveys, case-control studies, and cohort studies, though plenty of epidemiologists design and implement RCTs. [They do not typically distinguish between quasi-experimental and other non-experimental (or observational) designs like case-control or cohort studies.] Figure 5.6: Common observational designs Observational studies can yield important insights about cause and effect, but they have limitations. You’ve probably heard that correlation does not equal causation. For instance, did you know there is a nearly perfect correlation between the per capita consumption of cheese and the number of people who have died by becoming tangled in their bedsheets? (If you just put down the hunk of aged cheddar you were eating, please keep reading this book!) That said, all studies have limitations and trade offs. Designing a good study is a process of weighing scientific objectives with logistical constraints, ethical considerations, time, money, and a host of other factors. Here are some common observational designs. Cross-Sectional Descriptive research The goal of descriptive research is to characterize the population. Often this means estimating the prevalence of a phenomenon or disease. 20% are illiterate. 36% have an unmet need for contraception. 9% are HIV positive. Description can also be qualitative in nature (e.g., thick description). Just about every study will have some descriptive element. Some studies are primarily descriptive. A good example are the Demographic and Health Surveys, more commonly referred to as DHS surveys (yes, “surveys” is redundant). Every student of global health should come to know what the DHS Program has to offer. The program is funded by the U.S. Agency for International Development (USAID), and registered users can request access to data from more than 300 surveys conducted in 90+ countries. A DHS survey is also an example of a cross-sectional study. These are typically one-off surveys but can include other forms of data collection. The key is that it’s a snapshot. The goal is often description but might also include correlation. Cross-sectional studies are differentiated from panel or longitudinal studies by their participants; the latter include the same research participants (sample) over time in multiple studies, whereas cross-sectional studies only include a particular sample once. So even though the DHS Program will conduct a new survey in a country every five years or so, they always recruit a new sample of participants (a.k.a. “successive independent samples”). This makes the DHS surveys cross-sectional rather than panel or longitudinal in design. DHS surveys are a good example of demographic research. Demographers contribute to and use data sources like DHS surveys and national population and housing censuses to understand more about population size, structure, and change (e.g., birth, death, migration, marriage, employment, education). Many countries strive to conduct a census, or an enumeration of all citizens, every 10 years. The United Nations Statistics Division and the United Nations Population Fund (UNFPA) provide technical support (a.k.a., help) to countries preparing for, conducting, and analyzing a national population and housing census. These two organizations, in partnership with the United Nations Children’s Fund (UNICEF), maintain CensusInfo, a database of global census data. Here is the relevant table from the 2014 Kenya DHS Key Indicators Report for describing the prevalence of ITN use.32 This is a typical DHS cross tabulation (or crosstab) of the results. In this example, the percentage of children under the age of 5 that slept under an insecticide treated net the previous night in Kenya was 54.1%. This descriptive data is further disaggregated by residence and wealth quintile as is typical for DHS tables.33 Figure 5.7: Source: Kenya 2014 DHS Key Indicators Report, http://bit.ly/1g4NYS5 The data summarized in this table describes the problem of bed net use. Descriptive questions are well-suited for needs assessments. Before we can design a program or policy to increase bed net usage, for instance, we must to understand the need. In Kenya, almost half of children under 5 are not sleeping under insecticide treated nets according to the DHS. This is a particular concern for children living in areas of high risk. Correlational research This descriptive information sheds light on programmatic and policy priorities, but we have to go beyond describing the problem to make a difference. A helpful next step is often to build on descriptive insights by attempting to predict or explain the behavior or phenomenon. For instance, Noor et al. (2006) asked a correlational research question (edited below) about the factors associated with net use among children under the age of 5: Are wealth, mother’s education, and physical access to markets associated with the use of nets purchased from the retail sector among rural children under five years of age in four districts in Kenya? Correlational research asks questions about the relationship (a.k.a. association) between two or more variables. In this case, ITN use and a variety of potentially influential factors, such as household wealth and a mother’s education level. Noor and colleagues reported that only 15% of children in the rural study sample slept under a net the previous night—a much lower percentage than the national prevalence reported by recent DHS surveys. As shown in the table below, they also found that several factors were associated with higher odds of bed net use, including: greater household wealth, living closer to a market center, not having older children present in the household, having a mother who is married and not pregnant, being younger than 1 year old, and having an immunization card. Figure 5.8: Source: Noor et al. (2006), http://bit.ly/1HoltVo Cohort A cohort is a group of people recruited because they share something in common. In a prospective cohort study, participants without the outcome of interest are recruited and followed into the future for a period of time to see who develops the outcome. For instance, Lindblade et al. (2015) conducted a prospective cohort study in Malawi to test the efficacy of ITNs in an area of moderate resistance to pyrethroids, a common class of insecticide. They followed a cohort of 1,199 healthy children (i.e., no malaria) aged 6-59 months for one year and found that the incidence of malaria infection over this time was 30 percent lower among ITN users compared to non-users. The exposure in this study was bednet use the night before the study team visited the household: ITN user, untreated net user, or no net. This is a bit confusing because using bednets is protective. Most often we read about exposures that are bad, like smoking. The research question is the same in both cases, however: does the emergence of the outcome differ by exposure status? This is promising, but the study design has limitations. One important limitation is that the children were not randomized to ITN access. So it could be the case that children who used the ITNs were somehow different from the children who did not use the ITNs. This is a potential selection bias, a threat to internal validity. You’ll learn more about such threats in a later chapter. The basic challenge for causal inference is that the design does not rule out the possibility that something other than ITN use accounted for the reductions in malaria infections. A retrospective cohort study looks similar except that, by the time the researcher gets involved, the cohort has already been recruited and data collected. For instance, a researcher might use medical records to find groups of people who share many characteristics but differ in terms of some exposure (e.g., smoking). These groups are then compared to see who went on to develop the outcome. For instance, Fullman et al. (2013) used birth history information contained in DHS and MIS micro-data (i.e., survey data about individuals and households, not just the summary reports) to construct retrospective cohorts of children from age 1 to 59 months. They were interested in estimating the effect of ITNs and indoor residential spraying (the exposures) on child mortality (the outcome) within 59 months of birth, so they used survey data to determine if and when households were “exposed” to these prevention tools. Note that everything about this study was retrospective; the authors did not collect their own data or follow any participants over time. In the end, they found that bednets and spraying reduced malaria-related morbidity, but not child mortality. Case-control Sometimes it is not possible to recruit a group of healthy people and wait to see who gets sick as you would in a prospective cohort study. Imagine having to wait a decade or more to see who develops rare diseases like gliomas. This would be a very expensive study that would need to involve thousands of people to study such a rare disease that takes time to emerge. A case-control study might be a better fit. In this design, researchers identify people with the disease (cases) and without the disease (controls) and ask them about past exposures. Obala et al. (2015) did this in Kenya with 442 children hospitalized with malaria and healthy matched controls without evidence of malaria. They wanted to know why there is a high malaria burden despite high ITN coverage. The research team visited visited the home of each case and control and asked questions about ITN coverage and recent use, along with measuring the parasite burden of family members, mapping nearby potential vector breeding sites, and assessing neighborhood ITN coverage. Obala and colleagues found that ITN coverage was not correlated with hospitalizations, but consistent ITN use decreased the odds of hospitalizations by more than 70%. As with prospective cohort designs, there is a risk of selection bias. In this case, we have to be concerned that the matching process was not perfect. The matching was done on the basis of age, gender, and village. But there could be unmeasured ways in which the cases and controls differ, which would undermine the results. A case-control study looks like a retrospective cohort, but flipped: In a retrospective cohort you look to see if people with different exposures have different outcomes. In a case-control study you look to see if people with different outcomes had different exposures. Another difference is that, in a case-control study, the researcher recruits participants in the present day and asks them about historical events, whereas all data collection has already taken place in a retrospective cohort study. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["logic.html", "6 Theory of Change 6.1 All Models are Wrong, But Some Are Useful 6.2 Theory of Change 6.3 Logic Model Share Feedback", " 6 Theory of Change Underneath any good claim of causal inference is theoretical model of how we think X actually causes Y. Few studies set out to test a specific mechanism of impact, but most impact evaluations are designed around a theory about how the world works. We call this a theory of change. A theory of change articulates how an intervention—or a policy, program, or treatment—is expected to impact an outcome. It explains how X causes Y, and what is needed for this to happen. You might see this referred to as a theory of change, logic model, logical framework, causal model, results chain, pipeline model, results framework, program theory, or one of several other combinations of these words. Developing a good theory of change is an essential part of designing effective programs, but I like to teach students about these models because they help us think through measurement issues. Before you can articulate how you will measure key study variables, you need to think through what these variables are in the first place. A theory of change can help you approach this task. 6.1 All Models are Wrong, But Some Are Useful Before I describe how to create a theory of change, I want to frame this discussion in a broader context. A theory of change falls under the larger umbrella of what are known as conceptual models. A model is a simplified representation of a more complex reality. A plastic replica of the human heart is a model. So is an epidemic model of Ebola tranmission. Neither model is perfect, but we can probably learn something from each one. As the mathemetician and statistican George Box famously wrote, “Essentially, all models are wrong, but some are useful” (Box and Draper 1987). Conceptual models are used to propose how constructs are related—without necessarily claiming that the relationships are causal. You’ll come across many different types of conceptual models in the literature. Let’s review a few examples before turning to theories of change. 6.1.1 IN PSYCHOLOGY AND RELATED DISCIPLINES Conceptual models are very common in psychology, public health, and related fields. One of the most commonly used conceptual models in the study of health behavior change is the health belief model (Figure 6.1). The health belief model was developed in the 1950s as researchers in the U.S. Public Health Service were trying to understand why people were reluctant to engage in preventive health behaviors (Rosenstock 1974). This model suggests that health behaviors are explained by a person’s perception of the benefits vs. risks of action, perceived threat of the health issue, self-efficacy for change, and cues to action. Figure 6.1: Health belief model. Source: http://bit.ly/2i9Lw0Ehbm.jpg Another example is the information-motivation-behavioral skills (IMB) model proposed by Fisher and Fisher (1992) to explain HIV-related risk behaviors (Figure 6.2). Figure 6.2: Information-motivation-behavioral skills model Conceptual models like this one can be tested empirically (i.e., with data). For instance, Zhang et al. (2011) explored the fit of the IMB model to data on condom use among female commercial sex workers in China. Figure 6.3 is a path diagram that shows the results of a structural equation model that was fit to the data. Figure 6.3: Information-Motivation-Behavioral Skills structural equation model. Source: Zhang et al. (2011), http://bit.ly/2i2SrWb Structural equation modeling (SEM) is a multivarite technique that combines a sturctural model represented by the path diagram and a measurement model that specifies the relationships between latent variables and their indicators. In path diagrams like the one shown in the figure above, directly observed variables are called manifest variables and are represented as rectangles, and latent constructs are represented as elipses. For instance, condom use is a manifest variable that is directly measured, whereas self-efficacy is a latent construct that is measured indirectly by asking people several questions that are combined to make a composite indicator of this thing called self-efficacy. In the language of SEM, variables are labeled endogenous and exogenous rather than independent and dependent. In this example, knowledge and perceived risk are exogenous variables because they are not “caused by” another variable. Condom use is an endogenous variable. 6.1.2 IN EPIDEMIOLOGY Directed acyclic graphs A related type of causal diagram are directed acyclic graphs, or DAGs. This idea from mathematics and computer science has been applied to epidemiologic (observational) research in an effort to identify potential confounding variables that need to be addressed to make valid causal inferences (Greenland, Pearl, and Robins 1999). in Figure 6.4 shows two example DAGs. In this example, Suttorp et al. (2015) show that, at a minimum, it is necessary to control for age when estimating the relationship between chronic kidney disease and mortality (a). If cancer is directly related to kidney disease (b), it will also be important to control for cancer. Drawing out these relationships helps us to see that it may not be necessary to control for dementia because there is no direct relationship between dementia and kidney disease. Figure 6.4: Graphical presentation of confounding in directed acyclic graphs. Identification of a minimal set of factors to resolve confounding. In (a), the backdoor path from CKD to mortality can be blocked by just conditioning on age, as depicted by the box around age. However if we assume that cancer also causes CKD (b), the backdoor paths can only be closed by conditioning on two factors, either age and cancer (as depicted) or cancer and dementia. Source: Suttorp et al. (2015), http://bit.ly/2i5ZZwq Epidemic model Epidemic models are used to explain or predict the spread of an epidemic. Kermack and McKendrick (1927) proposed a deterministic compartmental model called SIR that consists of the number of uninfected people susceptable to the disease (S), the number of infected (I), and the number of people removed (R) through death or immunization. Rivers et al. (2014) used this basic framework to create a compartmental model of the 2014 Ebola outbreak in Liberia and Sierra Leone (Figure 6.5): Susceptible (S) Exposed (E) Infectious (I) Hospitalized (H) Funeral (F; handling bodies) Recovered/Removed (R) Figure 6.5: Compartmental flow of a mathematical model of the Ebola Epidemic in Liberia and Sierra Leone, 2014. Source: Rivers et al. (2014), http://bit.ly/2jkpl5G Rivers et al. (2014) published this model in November 2014, concluding: The forecasts for both Liberia and Sierra Leone in the absence of any major effort to contain the epidemic paint a bleak picture of its future progress, which suggests that we are in the opening phase of the epidemic, rather than near its peak. The key phrase is in the absence of any major effort to contain the epidemic. It turned out that November 2014 was actually the peak of new cases thanks to a coordinated effort respond to the outbreak. As the authors wrote at the time: Of the modeled interventions applied to the epidemic, the most effective by far is a combined strategy of intensifying contact tracing to remove infected individuals from the general population and placing them in a setting that can provide both isolation and dedicated care. This intervention requires that clinics have the necessary supplies, training and personnel to follow infection control practices. Although both of these interventions in isolation also have an impact on the epidemic, they are much more effective in parallel. 6.1.3 IN STATISTICS Statistical models are non-deterministic and thus incorporate stochastic variables. In other words, some of the variables being modeled have probability distributions rather than constant inputs that you might see in physics or mathmatics. Statistical models are typically communicated as a set of equations and visualized as a set of results. An exception is the path diagram represented visually in Figure 6.3. 6.1.4 IN ECONOMICS Economic models can be stochastic or non-stochastic. The field of econometrics shares a lot in common with statistics, including a focus on stochastic models. Economics more broadly, however, also uses non-stochatic models. For instance, Figure 6.6 shows the hypothesized mathmatical relationship between mortality and income (Wildman and Shen 2014). This graph is based on theory and does not plot actual empirical data collected in a particular setting. Figure 6.6: Effect of increased inequality on population mortality. Source: Wildman et al. (2014), http://bit.ly/2iUFqA9 6.2 Theory of Change Theory of change diagrams come in a variety of flavors. There is no RIGHT WAY™ to create one, as long as you can convey the fundamentals of how you think X leads to Y. If people understand your diagram, it is a good diagram. People are more likely to understand your diagram and your theory of change if you include a few common elements. The United Kingdom’s Department for International Development, commonly known as DFID or UK Aid, commissioned a report on the uses of theories of change in international development that identified several common components:34 influence of context discussion of long-term change process/sequence of change explained underlying assumptions presented as a diagram and narrative summary 6.2.1 TEMPLATE My preferred approach to outlining a theory of change is to follow this template from the W.K. Kellogg Foundation. Here is an editable PowerPoint version. Figure 6.7: Theory of change template. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay Start with the (research) problem statement (Box 1). This box gets at the heart of the reason your intervention exists. What are you trying to solve? You’d be surprised how often program design is disconnected from the problem that led to the program in the first place. This planning process will help you prevent you from ending up in this situation. Next think about what assets already exist and what needs remain (Box 2). There is always something to build upon, which is why it’s important to look for strengths in addition to challenges. This process is best done in collaboration with people impacted by the problem so that the proposed solution is grounded in their reality. If available, descriptive data sources like the DHS can help to outline (1) and (2). For a more local perspective, it might be beneficial to conduct a brief needs assessment in partnership with the local community if resources permit. Box 3 jumps to the desired results. If the program works, what will change? We’ll come back to what is meant by “outputs, outcomes, and impact” in a moment. With the results articulated, you can go back to the beginning and consider what factors might affect your program’s success positively or negatively (Box 4). The next step is to outline strategies for achieving your desired results that take into account potential barriers and facilitators (Box 5). This is where you articulate what your program will actually do. Finally, Box 6 reminds you that every theory of change is built on a set of assumptions. Be thorough and transparent as you think through the hidden beliefs that underlie your ideas about how your program will achieve its results. 6.2.2 EXAMPLE: KENYAN “SUGAR DADDIES” In 2014, an estimated 1.4 million people in Kenya were living with HIV. This is a prevalence rate of 5.3% among adults aged 15 to 49. Without a cure for AIDS, prevention remains critical to ending the epidemic. Starting in 2001, Kenya integrated HIV/AIDS education into the primary school curriculum as a new prevention strategy (JPAL 2007). At the time, the focus of this program—and many others programs across sub-Saharan Africa—was complete risk avoidance (Dupas 2011). Abstinence. Information on risk reduction was limited. In particular, students were not learning about the differential prevalence of HIV infection by age and gender. Girls were not learn that the older “sugar daddies” who provide nice things like phones and airtime in return for sex are more likely than the girls’ goofy age mates to be infected. An organization called ICS Africa set out to change this by rolling out a “Relative Risk Information Campaign” in Kenya. The intervention was brilliant in its simplicity. A program staffer would talk with students for 40 minutes. During this time, the staffer showed the class a 10-minute video on sugar daddies and led a discussion about cross-generational sex. During the session, the staffer reviewed results of recent studies and wrote facts about the distribution of HIV prevalence on the chalk board. JPAL researchers tested ICS Africa’s risk reduction strategy in a randomized experiment in Western Kenya. In the first phase (2003), 328 schools were randomized to teacher training on the national HIV prevention curriculum (Duflo et al. 2006). In the second phase (2004), 71 of these schools were stratified and randomized to receive the sugar daddy intervention (Dupas 2011). In total there were four study arms: (i) teacher training only, (ii) sugar daddy only, (iii) teacher training and sugar daddy, and (iv) nothing. The results were shocking. Teacher training was a bust. Sure, the training led to a change in teaching practices—notably that trained teachers mentioned HIV in class more often than non-trained teachers—but it had little effect on HIV knowledge or childbearing. In contrast, the 40-minute sugar daddy discussion and video reduced childbearing with men at least five years older by 65%—and not because girls started having babies with males their own age. The overall incidence of childbearing fell by 28%. With a cost of $28.20 USD per school and $0.80 per student, the cost per childbirth averted was $91 (JPAL 2007). Returning to our template, let’s piece together the theory of change for the relative risk reduction program. Figure 6.8: Sugar daddy awareness theory of change Increasing knowledge about HIV makes intuitive sense as an outcome for a study about HIV prevention. But why childbearing? Well, babies don’t come from storks. They come from unprotected sex. It’s harder to lie about birthing a baby than it is to lie about your private sexual behavior, so childbearing is thought to be a more objective measure of unprotected sex. Unprotected sex is a main driver of HIV transmission, so measuring childbearing is a proxy for HIV risk from unprotected sex. 6.3 Logic Model While a theory of change tends to be a high-level depiction of the “why”, a logic model—or logframe—is more detailed and focuses on the “how”. Logical models are a useful tool for program planning, monitoring program implementation, and program evaluation and reporting. You’ll often see them presented in the “results chain” or “pipeline” format shown in Figure 6.9. In a logic model, inputs and activities represent your planned work. Outputs, outcomes, and impact are your intended results. Figure 6.9: Logic model. Source: W.K. Kellogg Foundation, http://bit.ly/1My75Ay Table 6.1: Components of a logic model. Component Description Input The resources needed to implement the program. People, money, time, etc. Activities What your program will do. Trainings, events, distribution of goods, etc. Outputs What your program did. Number of people trained, number of events held, number of goods delivered and number of people who benefitted. Outcomes Short- and medium-term results of your program. Increased knowledge, decreased risky behavior, improved functioning, etc. Impacts Long-term effects of the program outcomes. Lower HIV prevalence, reduced morbidity and mortality, etc. Figure 6.10 shows what a logic model might look like for the Kenyan relative risk reduction program introduced earlier. Figure 6.10: Sugar daddy awareness logic model As with a theory of change, there is no RIGHT WAY™ to create a logic model, but you will often see the same basic components (inputs, activities, outputs, outcomes, impacts) included. Figure 6.11 shows an example logic model for evaluating reproductive health programs. Figure 6.11: Reproductive health logic model. Source: MEASURE Evaluation, http://bit.ly/2iwphA7 The next step is to think through how to measure key variables throughout your conceptual model, theory of change, or logic model. Donors like USAID commonly require project staff to create results frameworks to monitor progress toward achieving the stated goals. Here’s an example results framework from the Feed the Future strategic plan for Kenya. You can read this diagram from the top down. The overall goal reflects the mission of the broader FTF program: reduce poverty and hunger. The plan for attaining this goal is to reach two objectives: inclusive agricultural sector growth and improved nutritional status. If the goal represents the ultimate desired impact, objectives are long range strategies. You know you are on track to reaching your objectives if you hit several intermediate results, or IRs. For instance, IR6, “Improved utilization of MCH and nutrition services”, is expected to improve the nutritional status of women and children. This IR has three sub IRs, including 6.3 “Strengthened MCH nutrition surveillance”. The rationale for sub-IR 6.3 is that better monitoring and data will enable earlier identification of at-risk individuals, which in turn should mean earlier initiation of nutrition interventions. Every IR has a set of indicators for measuring progress. In the case of IR6, FTF lists the following illustrative indicators: prevalence of maternal anemia number of children under five years of age who received vitamin A from USG-supported programs number of people trained in child health and nutrition through USG-supported health area programs (disaggregated by gender) number of clients who received food and/or nutrition services (PEPFAR indicator) Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["indicators.html", "7 Measurement 7.1 Terminology 7.2 Identify Constructs 7.3 Select Good Indicators 7.4 Constructing Indicators 7.5 Indicators Throughout the Causal Chain Additional Resources on Indicators Share Feedback", " 7 Measurement In the last chapter you learned that outcomes are the intended results of your program. In this chapter you’ll learn how to go from outcomes to indicators and how to specify a measurement plan for all parts of your logic or conceptual model. 7.1 Terminology Regardless of what research design you are using, if your goal is to estimate the impact of a program/intervention/treatment/policy on [blank], [blank] is your outcome. But what is the difference between an outcome and an indicator? Between an indicator and an instrument? Glennerster and Takavarasha (2013) provide helpful definitions of the terms you’ll come across in discussions of measurement and data collection. I expand on their list in the table below. Term Definition Example Construct A characteristic, behavior, or phenomenon to be assessed and studied. Often cannot be measured directly. depression Outcome In an impact evaluation, ‘constructs’ will be referred to as outcomes—the intended results of your program. Also referred to as endpoint in a trial. decreased depression Indicator Observable measures of outcomes or other study constructs. depression severity score on a depression scale Instrument The tools used to measure indicators. Also referred to as a measure. a depression scale, made up of questions/items about symtoms of depression Variable The numeric values of the indicators. Respondent The person (or group) that we measure. Let’s work through an example to highlight the terms. Patel et al. (2016) designed a RCT in India to test the efficacy of a lay counsellor-delivered brief psychological treatment for severe depression. The hypothesized outcome was a reduction in severe depression. In a theory of change or logic model, outcomes take on the language of change: increases and decreases. But you’ll also see the word “outcome” used more generally, and synonymously with “indicator”, particularly in articles reporting study results. For example, Patel et al. (2016) write: Primary outcomes were depression symptom severity on the Beck Depression Inventory version II and remission from depression (PHQ-9 [Patient Health Questionnaire] score of &lt;10) at 3 months in the intention-to-treat population, assessed by masked field researchers. Using the language in the table above, we could also say that the primary outcome was severe depression and the team measured two indicators of severe depression: (i) a depression symptom severity score on the Beck Depression Inventory version II (BDI-II) and (ii) a score of less than 10 on the PHQ-9. They used two instruments to measure depression: the PHQ-9 (pdf) and the BDI-II. Outside of the impact evaluation literature, the word “outcome” will often be replaced with “dependent variable” or “response variable”. Additional constructs of interest might be called “covariates”, “independent variables”, or “exposure variables”. To make matters simple, I recommend asking yourself the following questions when you are planning a study: What is the key construct you want to study? What other constructs do you need to measure at the same time to fully understand your key construct? What are indicators for these constructs? In other words, how will you quantify these constructs? How will you measure these quantities? What instruments will you use? (This is the topic of the next chapter.) Fundamentally, there should be a logical flow from your research problem to your measurement of primary study outcomes/constructs. Figure 7.1 demonstrates this using Patel et al. (2016) as an example. Figure 7.1: From research problem to study instruments The language of qualitative studies is a bit different. There is an emphasis on study constructs, but not on indicators or measures. Quantification is not the goal. 7.2 Identify Constructs Most studies are designed to provide the best evidence possible about one or two primary outcomes linked directly to the main study objective. Secondary outcomes may be registered, investigated, and reported as well, but these analyses could be more exploratory if the study design is not ideal for measuring these additional outcomes. For instance, Patel et al. (2016) included the following secondary outcomes in addition to depression severity and remission from depression: Secondary outcomes were disability on the WHO Disability Assessment Schedule II and total days unable to work in the previous month, behavioural activation on the five-item abbreviated Activation Scale based on the Behavioural Activation for Depression Scale-Short Form, suicidal thoughts or attempts in the past 3 months, intimate partner violence (not a prespecified hypothesis), and resource use and costs of illness estimated from the Client Service Receipt Inventory. These outcomes were labeled secondary because the study was powered on the primary outcomes (a topic of a later chapter): …we aimed to recruit 500 participants to detect the hypothesised effects (a standardised mean difference of 0·42), with 90% power for the primary continuous outcome of depression severity and 92% power to detect a recovery of 65% in the HAP group for our primary binary outcome of depression remission. The basic idea is that one study cannot definitively answer every possible research question. There are tradeoffs in terms of the time, money, and resources, and investigators must prioritize among all possible outcomes. 7.3 Select Good Indicators To define a study construct in terms of an indicator and to specify its measurement is to operationalize the construct. Indicators should be DREAMY™: Defined clearly specified Relevant related to the construct Expedient feasible to obtain Accurate valid measure of construct Measurable able to be quantified customarY recognized standard 7.3.1 DEFINED It is important to clearly specify and define all study variables, especially the indicators of primary outcomes. This is a basic requirement for a reader to critically appraise your work, as well as a building block for future replication attempts. For instance, the construct of interest in Patel et al. (2016) was severe depression, and the two indicators were (i) depression symptom severity and (ii) remission from depression. The authors pre-registered the trial and defined these outcomes as follows: Mean difference in total score measured by the Beck’s Depression Inventory (BDI-II), at 3 months, a 21-item questionnaire assessment of depressive symptoms; each item is scored on a Likert scale of f 0 to 3. It measures depression severity based on symptom scores. Remission, defined as a score of &lt;10 measured at 3 months by the Patient Health Questionnaire (PHQ-9), a nine-item questionnaire for the detection and diagnosis of depression based on DSM-IV criteria. It is scored on a scale of 0 to 3 based on frequency of symptoms. 7.3.2 RELEVANT Indicators should be relevant to the construct of interest. In Patel et al. (2016), scores on the BDI-II and PHQ-9 are clearly measures of depression severity and remission. An example of a non-relevant indicator would be scores on the Beck Anxiety Inventory, a measure of anxiety. While anxiety and depression are often co-morbid, anxiety is a distinct construct. 7.3.3 EXPEDIENT It should be feasible to collect data on the indicator given a particular set of resource constraints. Asking participants to complete a 21-item questionnaire and a 9-item questionnaire as in Patel et al. (2016) does not represent a large burden on study staff or participants. However, collecting and analyzing biological samples such as hair, salavia, or blood might. 7.3.4 ACCURATE Accurate is another word for “valid”. Indicators must be valid measures of study constructs. In other words, we need to be sure that scores on the BDI-II and PHQ-9 measure this thing we’re calling depression. I’ll discuss this in more detail shortly. 7.3.5 MEASUREABLE Indicators must be quantifiable. Psychological constructs like depression are often measured through the use of scales such as the BDI-II and the PHQ-9. Other constructs require more creativity. For instance, Olken (2005) measured corruption in Indonesia by digging core samples of newly build roads to estimate the amount of materials used in construction and then compared cost estimates against reported expenditures to get a measure of corruption (i.e., missing expenditures). 7.3.6 CUSTOMARY Whenever possible, it’s smart to use standard indicators and follow existing definitions and calculation methods. One way to learn about standards and customs is to keep up with the literature and find articles that measure the same constructs. If you want to publish the results of your impact evaluation of a microfinance program in an economics journal, read other papers by economists. How do they measure outcomes like income, consumption, and wealth? Study measurement is not a good opportunity to show your creative side unless you are explicitly trying to overcome limitations to the standard methods. If you are studying population-level issues, it’s likely that you can find your indicator in the World Health Organization’s Global Reference List of the 100 core health indicators (WHO 2015a): Figure 7.2: WHO 100 core health indicators. Source: http://bit.ly/1NgGeLh If 100 is not enough indicators for you, try the United Nations Sustainable Development Goals. There are 230 indicators to measure 169 targets for 17 goals! The SDG indicators even get their own website. Figure 7.3: Sustainable Development Goals. Source: http://bit.ly/2cuDpWN. 7.4 Constructing Indicators 7.4.1 SINGLE ITEM INDICATORS Some indicators are measured with responses to a single item (or a short series of items) on a survey. For instance, in Malaria Indicator Surveys the “proportion of households with at least one ITN” is the “number of households surveyed with at least one ITN” (numerator) divided by the “total number of households surveyed” (denominator). The numerator for this indicator is obtained from asking the household respondent if there is any mosquito net in the house that can be used while sleeping and from determining whether each net found in a household is a factory-treated net that does not require any treatment (an LLIN) or a net that has been soaked with insecticide within the past 12 months. The denominator is the total number of surveyed households. To determine if a household has an ITN, enumerators ask the following sequence of questions. Number Question 119 Does your household have any mosquito nets? 120 How many mosquito nets does your household have? 121 ASK THE RESPONDENT TO SHOW YOU ALL THE NETS IN THE HOUSEHOLD 122 How many months ago did your household get the mosquito net? 123 OBSERVE OR ASK BRAND/TYPE OF MOSQUITO NET 124 Since you got the net, was it ever soaked or dipped in a liquid to kill or repel mosquitoes? 125 How many months ago was the net last soaked or dipped? The end result is a binary indicator (yes/no) of whether the household has a bednet that has been dipped in the past 12 months or is factory-treated. In theory it is possible to ask this in one question—“Does your household have any factory-treated mosquito nets or nets that have been dipped in a liquid to kill or repel mosquitoes in the past 12 months?—but this is a long and complicated question. It’s more effective to break up the question. Sometimes more abstract constructs can also be measured with just one item. For instance, Konrath et al. (2014) ran 11 studies and found that you can measure narcissism with one question: To what extent do you agree with this statement: “I am a narcissist.” Response options range from “not very true about me” (1) to “very true of me” (7).35 Most often, however, constructs like narcissism and depression are measured with multiple items that are combined into indexes or scales. These two terms are often used interchangeably, but they are not synonyms. While sharing in common the fact that multiple items or observations go into their construction—making them composite measures—the method and purpose of combining these items or observations is distinct. 7.4.2 INDEXES Indexes combine items into an overall composite, often without concern for how the individual items relate to each other. For instance, the Dow Jones Industrial Average is a stock market index that represents a scaled average of stock prices of 30 major U.S. companies such as Walt Disney and McDonald’s. The Dow Jones is a popular indicator of market strength and is constantly monitored during trading hours. Every index has its quirks, and the Dow Jones is no exception. Companies with larger share prices have more influence on the index. An index popular with the global health crowd is the DHS wealth index. As a predictor of many health behaviors and outcomes, economic status is a covariate in high demand. Failing to measure economic status in a household survey would be like failing to note a respondent’s gender or age, but measuring economic status is not nearly as easy.36 In an ideal data world every survey would include accurate information on household income and consumption as measures of household wealth. Income is volatile, however, and consumption is very hard to measure quickly. So in the late 1990s, researchers first proposed creating an index of household assets as a measure of a household’s economic status (Rutstein and Johnson 2004). Data for the wealth index come from DHS surveys conducted in a particular country. Indicator variables include individual and household assets (e.g., phone, television, car), land ownership, and dwelling characteristics, such as water and sanitation facilities, housing materials (i.e., wall, floor, roof), persons sleeping per room, and cooking facilities. The figure below shows a snapshot of the DHS Household Questionnaire. Figure 7.4: DHS Round 7 Household Questionnaire. A key decision when creating indexes like the wealth index is whether to weight the individual components. Should owning a car be given the same weight as owning a phone? In other words, in constructing an index that measures someone’s wealth, should owning a phone contribute as much to the index as owning a car? Most readers would probably say no, so the next question is how to assign differential weights to the components. Filmer and Pritchett (2001) first proposed assigning weights via principal components analysis, or PCA. PCA is a data reduction technique in which indicators are standardized (i.e., transformed into z-scores) so that they each have a mean of 0 and a variance of 1. If there are 10 items, the total variance is therefore 10, and there are 10 principal components. A principal component (aka eigenvector) is a linear combination of the original indicators, so every indicator (e.g., yes/no to owning a phone) has a factor loading that represents the correlation between the individual indicator and the principal component. The first principal component always explains the most variance, and each component after the first explains a smaller and smaller amount of total variance. In constructing the wealth index, we assume that the first component measures this thing called “wealth”, so we use the factor loadings on the first principal component to create a score for each household. Let’s use the 2014 Bangladesh DHS survey as an example. Figure 7.5: Example wealth index construction (abbreviated from 2014 Bangladesh DHS). As shown above, the factor loading for water piped into a dwelling (i.e., indoor plumbing) was 0.056 in the PCA run on the 2014 Bangladesh DHS data. In order to create the index, this loading gets converted into a score for whether the household has or does not have the asset, and these indicator scores are summed to get an overall index score for each household. Once every household has an index score, it’s possible to assign every participant to of 1 of 5 wealth quintiles reflecting their economic status (relative to the sample). This makes it possible to examine the relationship between health outcomes and wealth. 7.4.3 SCALES In an index, indicators “cause” the concept that is being measured. For instance, a household’s wealth is determined by the assets it owns (e.g., livestock, floor quality). Conversely, in a scale, the concept “causes” the indicators. Figure 7.6: Scale vs index. Let’s take an example like depression. There is not a blood test for depression, so depression is a construct that needs a definition. According to the Diagnostic Criteria for Major Depressive Disorder and Depressive Episodes, currently the DSM-V, the criteria for Major Depressive Disorder are as follows, A-E: A. Five (or more) of the following symptoms have been present during the same 2-week period and represent a change from previous functioning; at least one of the symptoms is either (1) depressed mood or (2) loss of interest or pleasure. Depressed mood most of the day, nearly every day, as indicated by either subjective report (e.g., feels sad, empty, hopeless) or observation made by others (e.g., appears tearful). Markedly diminished interest or pleasure in all, or almost all, activities most of the day, nearly every day (as indicated by either subjective account or observation.) Significant weight loss when not dieting or weight gain (e.g., a change of more than 5% of body weight in a month), or decrease or increase in appetite nearly every day. Insomnia or hypersomnia nearly every day. Psychomotor agitation or retardation nearly every day (observable by others, not merely subjective feelings of restlessness or being slowed down). Fatigue or loss of energy nearly every day. Feelings of worthlessness or excessive or inappropriate guilt (which may be delusional) nearly every day (not merely self-reproach or guilt about being sick). Diminished ability to think or concentrate, or indecisiveness, nearly every day (either by subjective account or as observed by others). Recurrent thoughts of death (not just fear of dying), recurrent suicidal ideation without a specific plan, or a suicide attempt or a specific plan for committing suicide. B. The symptoms cause clinically significant distress or impairment in social, occupational, or other important areas of functioning. C. The episode is not attributable to the physiological effects of a substance or to another medical condition. D. The occurrence of the major depressive episode is not better explained by schizoaffective disorder, schizophrenia, schizophreniform disorder, delusional disorder, or other specified and unspecified schizophrenia spectrum and other psychotic disorders. E. There has never been a manic episode or a hypomanic episode. If someone meets criteria A-E, they are diagnosed with Major Depressive Disorder. A diagnosis by a trained mental health professional like a psychiatrist would usually be considered the gold standard measure of depression. Gold standards are in short supply in many places, however, and we need more feasible methods of measuring this thing called depression. A reasonable alternative is to develop a set of questions—a scale—that we could administer to someone as a way of measuring their symptom severity. Presumably, if a person scored high enough on this scale, we’d classify them as depressed. In this example, depression is the latent variable that we can’t measure directly. To get an indicator of depression, we need to measure a combination of manifest variables that are ‘caused’ by the latent variable depression. Patel et al. (2016) used the Beck Depression Inventory (Beck, Steer, and Brown 1996) to measure depression severity. The BDI-II consists of 21 groups of statements, such as: Sadness I do not feel sad I feel sad much of the time I am sad all the time I am so sad or unhappy that I can’t stand it Pessimism I am not discouraged about my future I feel more discouraged about my future than I used to be I do not expect things to work out for me I feel my future is hopeless and will only get worse Each item is a manifest variable—something that we measure directly by asking the question. The latent variable depression is measured indirectly by summing a person’s responses to all 21 manifest variables to create the BDI-II scale score. Determining the factor strucutre of scales Exploratory factor analysis Typically when developing a new scale, researchers will start with a large pool of potential items—many more than they could ever use an applied context where administration time is a relevant constraint—and use exploratory factor analysis or some other method of data reduction to shrink the pool. Exploratory factor analysis (EFA) looks a lot like PCA, but they are conceptually and computationally distinct. Whereas PCA results in a linear combination of indicators that maximized total variance, factor analysis maximizes the common or shared variance. Factor analysis helps us to understand the structure of the data. For instance, the BDI-II consists of 21 items that are thought to measure the latent construct of depression, but many studies have examined whether these items can be grouped into subfactors—different domains of depression. Manian et al. (2013) administered the BDI-II to 953 new mothers “from a large East coast metropolitan area” and then conducted EFA on data from half of the sample.37 They looked for 2- to 4-factor solutions and found that a 3-factor model made the most sense empirically (based on data) and theoretically (based on their knowledge of the literature). Their model suggested that the latent variable of depression is composed of three subfactors as shown in the figure below: cognitive symptoms, affective symptoms, and somatic symptoms. Confirmatory factor analysis Manian et al. (2013) then used the holdout data (i.e., data from their sample not used in the EFA) to test the fit of their 3-factor model through confirmatory factor analysis (CFA). It fit! The model is shown below. If you are using an existing scale in a new population or setting, CFA is a good technique to determine if the original factor structure generalizes to your context. A typical case is one in which the original scale is developed in a high-income setting and research suggests that it makes sense to construct an overall scale score (of some latent variable like depression) AND 2 or 3 subscale scores that correspond to subfactors like cognitive symptoms and affective symptoms. However, let’s say that you recruit a sample in a completely new context and your CFA suggests that the original 2-factor model does not fit. Maybe in a different cultural context depression is not manifested along the same dimensions of cognitive and affective symptoms. Such a finding should make you question whether it makes sense to use the scale as is without further evaluation of its applicability. Figure 7.7: Final 3-factor model of the BDI-II with standardized path coefficients. Source: Manian et al. (2013). Constructing scale scores An under-appreciated question when it comes to scales is how to actually construct scale scores. Let’s say you come up with a bunch of items that you think measure this thing—this latent construct—called depression, administer the survey with these items to a few hundred people, and conduct EFA and CFA to determine the factor structure. Now what? Well, you have a few options. DiStefano, Zhu, and Mindrila (2009) lump them into two buckets: refined and non-refined. Non-refined methods are most commonly used because they are simple to compute and easy to compare across samples. Sum raw scores. If there are 21 items each with a possible range of 0 to 3, you just add up the scores on each item. This is what the BDI-II does, which gives it a possible range of 0 to 63. Average raw scores. Same idea as summing, but averaging keeps the possible range consistent with the response scale. For instance, if you average 21 items with response options that range from 0 to 3, the possible scale scores will range from 0 to 3. Some people think this makes more intuitive sense when presenting results. Sum standardized scores. With this method you would first standardize each item to have the same mean and standard deviation. It might be a good idea if the standard deviations of the items vary quite a bit. Refined methods may produce more exact scores since items are weighted empirically (vs equal weighting in non-refined methods) and relationships between factors are reflected in the scoring, but they are more complex and require the analyst to make a number of decisions along the way that can produce very different results. 7.4.4 EVALUATING PSYCHOMETRICS When it comes to evaluating scales like the BDI-II, we often look at several psychometric properties that we can group generally into two buckets: reliability and validity. Here’s a simple example that highlights the basic difference between these terms. Imagine your bathroom scale. If you stepped on, then off, then on again, and the scale read 210 lbs and then 180 lbs, you would realize that you are the owner of a broken, unreliable scale. So you head to the store and pick up a new scale. You step on and off your new scale, and it reads 400.12 lbs then 400.15 lbs. It’s very reliable (good precision), but unfortunately very wrong (poor accuracy, invalid) since you actually weigh something closer to 195 lbs. Reliability A reliable instrument is a consistent instrument. Consistent over repeated use (as in the bathroom scale example), and consistent among it’s component parts. There are several methods for assessing the reliability of an instrument. Here are a few common approaches. Test-retest reliability This is sometimes referred to as stability. Participants complete your survey today and then again in after a short period of time, maybe a few days or a week. If each person’s score is the exactly the same the second time, your instrument would be perfectly reliable. It won’t be, but you’ll hope for a high correlation coefficient (conventionally higher than 0.70). Beck, Steer, and Brown (1996) assessed the test-retest reliability of the BDI-II by giving the screening to 26 outpatients in Philadelphia at their first and second therapy sessions, approximately 1 week apart. They reported a test-retest correlation was 0.93. The tricky thing with test-retest reliability is knowing when to conduct the retest. Wait too long and scores will change because people change. Don’t wait long enough and you just get people repeating their answers from memory. Interitem reliability This is when responses to items in your instrument are consistent. If not, you have to wonder if they are measuring the same underlying construct of depression. 1. One approach to finding unreliable items in your instrument is to calculate item-total correlations. It’s easy: correlate responses on each item with the total scale score. Generally item-total correlations exceeding 0.30 are sufficient. Beck, Steer, and Brown (1996) reported that item-total correlations for the 21 BDI-II items ranged from 0.39 to 0.70 in the outpatient sample. 2. Another approach is Cronbach’s alpha, a measure of internal consistency. Beck, Steer, and Brown (1996) reported a coefficient alpha value of 0.92 for the outpatient sample. To understand Cronbach’s alpha, you have to understand that instruments are imperfect, even the BDI-II. Every person’s observed score—e.g., their total score on the BDI-II—is actually a function of their ‘true’ score (which we can’t know) plus some amount of measurement error. Cronbach’s alpha gives you an estimate of how much variance in people’s scores is measurement error. When you calculate Cronbach’s alpha in a program like R or Stata, behind the scenes the program does the equivalent of splitting the dataset into two halves over and over again and calculating the correlation between total scores for the first half with total scores for the second half. Cronbach’s alpha is the average of all possible correlation coefficients. Things to note about alpha: Alpha can range from 0 to 1 0.70 is a rough guide for the low-end of acceptable A value of 1 would indicate complete redundancy suggesting that the items are too similar! Alpha is sensitive to the number of items so a high alpha might just reflect that there are a lot of items included in the scale Alpha is not a property of the test, rather a characteristic of the test when used in a particular sample Alpha should not be used when a scale might tap different latent constructs—only use alpha when the scale is unidimensional Dunn, Baguley, and Brunsden (2014) and others reviewed the limitations of alpha and suggest coefficient omega as an alternative Interrater reliability This is another type of reliability that indicates whether two observers are consistent in observational ratings. Instead of using self-report instrument like the BDI-II, we might want to have two observers watch a video of a parent and child interacting and ‘code’ the parent’s behaviors using a depression rating system that we developed. If the observers agree a lot in their video ratings, they would be reliable. Things to note about interrater reliability: Percent agreement is one method of evaluating two raters when the rating is binary, but it does not account for agreement that can happen by chance Cohen’s kappa coefficient does account for agreement by chance; generally want a value greater than 0.40 Weighted kappa is good when the rating scale is ordinal (e.g., good &lt; better &lt; best) and you need to account for the fact that good vs best represents more disagreement than better vs best Intraclass correlation is a good option when you have 2 or more raters Validity Ask this three times: does your measure measure what you intend to measure with your measure? Or more simply: is the BDI-II an accurate measure this thing called ‘depression’? If not, it’s not a valid measure of depression. There are several types of validity that you can use to determine whether your instrument is valid. Face validity This is the weakest form of validity. The basic idea is: if it looks like a duck, quacks like a duck, it’s a duck. If people think your depression instrument asks about depression, then it has face validity as a measure of depression. This is a weak standard, however. A great looking instrument can perform very poorly in practice, and an instrument that appears to lack face validity might perform very well. The bottom line is that if you read an article and the only mention of validity is face validity, it’s a lame duck. Construct validity Depression is a hypothetical construct. If your new depression instrument has construct validity, it will be more strongly related to other instruments that are also thought to measure depression (convergent validity) and less strongly (or not at all related) to other instruments that claim to measure something other than depression (discriminant validity). For instance, Beck, Steer, and Brown (1996) reported that the BDI-II was more positively correlated with the Hamilton Psychiatric Rating Scale for Depression (0.71; convergent) than the Hamilton Rating Scale for Anxiety (0.47, discriminant). If your instrument has both convergent and discriminant validity, you have more confidence that it measures the construct you think it measures. Content validity This form of validity asks whether the components of an instrument—e.g., each question in a questionnaire—is relevant to the measurement of the larger construct. For instance, a question about difficulty sleeping is relevant to the measurement of depression since insomnia is a common symptom of depression. Conversely, a question about compulsive behaviors is probably not relevant since compulsive behaviors are not a typical symptom of the syndrome. Content validity also accounts for missing dimensions of a construct. If the BDI-II lacked a question about A5, “Psychomotor agitation”, we might question its content validity. Criterion-related validity An even more robust form of validity is criterion validity. Does an index test correctly classify people by their true disease status as determined by some gold standard criterion reference? Does a new rapid diagnostic test correctly identify evidence of malaria parasites in human blood samples? Do scores on the BDI-II correctly predict which people will be diagnosed with depression when evaluated independently by a mental health professional–the gold standard? Let’s use an example to explore criterion-related validity in more depth. Kim et al. (2014) conducted a study with 562 HIV-positive adolescents in Malawi in which they had the adolescents complete the BDI-II and then participate in a separate clinical interview with clinicians trained to use a structured interview tool called the Children’s Rating Scale, Revised (CDRS-R). Index test: With 21 items on the BDI-II—each with possible response values from 0 to 3—BDI-II scale scores can range from 0 to 63. Higher scores represent more severe depression. One goal of a validity study is to find a cutoff score on the index test that maximizes diagnostic accuracy. For instance, if the cutoff score is 15, anyone who scores greater than 15 is classified as depressed and everyone else is classified as not depressed. Criterion reference: In this study, independent clinician classification of depression was the gold standard. Every adolescent was classified as depressed or not depressed following a clinical interview. Taken together, there are four possible combinations of index test and gold standard results that we can display in a confusion matrix: true positive (a): both the test result and the gold standard indicate that the person is depressed true negative (d): both the test result and the gold standard indicate that the person is NOT depressed false positive (b): the test result suggests depression, but the gold standard disagrees false negative (c): the test result suggest no depression, but the gold standard disagrees Figure 7.8: Confusion matrix based on Kim et al. (2014). From this base set of numbers we can calculate a number of useful metrics about the index test (see the STARD guidelines for reporting in studies of diagnostic accuracy): Metric Details Prevalence This is the proportion of the sample (with the right methods we can infer to the population) who have (or had) a certain characteristic such as depression. We will distinguish between point, period, and lifetime prevalence, as well as introduce important issues related to sampling and prevalence, in a later chapter. Accuracy This is the total correct classification rate for a particular cutoff point. How often does the test correctly classify people according to their ‘true’ disease state measured by the criterion reference? 1-misclassification. [Area under the curve, or AUC, is another useful metric of test accuracy. See below for more details.] Sensitivity Sensitivity is also referred to as the true positive rate. Kim et al. (2014) reported an estimate of 0.75, which indicates that 75% of adolescents with depression correctly screened positive with a cutoff value of 15 False Negative Rate The flip side of sensitivity (true positives) is the false negative rate. In this example, 25% of depressed adolescents were misclassified by the test as not depressed. Specificity Specificity is also referred to as the true negative rate. Kim et al. (2014) reported an estimate of 0.77, which indicates that 77% of non-depressed adolescents correctly screened negative with a cutoff value of 15. False Positive Rate The inverse of specificity (true negatives) is the false positive rate. In this example, 33% of non-depressed adolescents were misclassified by the test as depressed. Positive Predictive Value If someone tests positive, you want to know how likely it is that the person is actually positive. This is what PPV indicates, but there’s a catch: the PPV (and NPV, below) depend on the prevalence of the condition in the sample, so they should only be used with representative samples obtained by probability sampling. Kim et al. (2014) report PPV, but this is questionable given their use of convenience sampling. If this were a probability sample, we could interpret a PPV of 0.43 to mean that the probability of depression when testing positive with the BDI-II using a cutoff of 15 is 43 percent. How is this different from sensitivity? Sensitivity does not depend on prevalence, but PPV does. A test can be sensitive (i.e., high true positive rate) but still produce many false positives if the prevalence of the condition is low, resulting in a low PPV. False Discovery Rate This is the inverse of PPV. Whereas PPV is the probability of disease if the test is positive, the FDR is the probability of a false positive (a false discovery) if the test is positive. Negative Predictive Value Similar to the PPV, the NPV asks if a person tests negative, what is the probability that they are actually negative. In the example shown, a NPV of 0.93 can be interpreted to mean that the probability of not being depressed when testing negative with the BDI-II using a cutoff of 15 is 93 percent. False Omission Rate This is the inverse of NPV. Whereas NPV is the probability of no disease if the test is negative, the FDR is the probability of a false negative (a false omission) if the test is negative. Positive Likelihood Ratio This is a measure of how much more likely a positive test result occurs in people with the condition compared to people without the condition. A LR+ of 1 would mean that the result is equally likely, therefore the test is not very helpful. In Kim et al. (2014) the LR+ is 2.3, which means that a positive test result is 2.3 times more likely if the person is actually depressed. This is fairly modest. As a clinician, you’d hope to see that the test had a LR+ ratio greater than 10 as a general rule. Negative Likelihood Ratio This is a measure of how much more likely a negative test result occurs in people with the condition compared to people without the condition. Again here a LR- of 1 signifies that the test is not useful. A ratio less than 1—particularly less than 0.1—is a good indicator that a negative result is diagnostically accurate. In the sensitivity and specificity calculations shown above, the example cutoff score for depression on the BDI-II was set to 15. To determine if 15 is the best cut point, we could calculate the sensitivity and specificity again for several different cutoff points and plot the relationship in a receiver operating characteristic (ROC) curve. ROC curves plot sensitivity (the true positive rate) against 1 minus specificity for a range of different cutoff points. For instance, the figure below shows the ROC curve presented in Kim et al. (2014). Receiver operating characteristic (ROC) curve for the BDI-II and CDI-II-S as compared to the CDRS-R. Source: Kim et al. (2014). The best cutoff is typically the one that maximizes sensitivity and specificity.38 In a ROC curve, this is the point that is closest to the top left of the graph—sensitivity of 1 and (1-specificity) of 0. In Kim et al. (2014), the optimal cutoff did indeed turn out to be 15. To evaluate the overall accuracy of the test, we can calculate the area under the ROC curve (AUC). In Kim et al. (2014), the AUC for the BDI-II is 0.82, generally considered to be an accurate benchmark (1=perfect; 0.5=worthless). The BDI-II performs better than an alternative screening instrument also assessed in this study called the Children’s Depression Inventory-II-Short (CDI-II-S); AUC=0.75. What is the difference between overall accuracy ((true positives + true negatives) / total) and AUC? Recall that the numbers you can calculate in a confusion matrix are dependent on the threshold you set for what counts as a positive test. Different thresholds (cutoffs) result in different patters of misclassification. AUC, on the other hand, takes into account sensitivity and specificity associated with all possible cutoffs. 7.5 Indicators Throughout the Causal Chain Indicators that define inputs, activities, and outputs in a logic model can be classified as process indicators. Process indicators capture how well a program is implemented. In short, the “M” (monitoring) in M&amp;E. As researchers we care about collecting good process/monitoring data in order to develop a better understanding why programs do or don’t work. For instance, we might want to track program costs so that we can estimate cost-effectiveness. Or we might want to know if the intervention was delivered according to plan or not. Researchers often rely on program partners to deliver the intervention under investigation, so issues like fidelity to treatment and compliance with study protocols are important to track closely. Let’s refer once more to Patel et al. (2016) to review a few examples of process indicators that intervention researchers often care about. Figure 7.9: Indicators throughout the causal chain. 7.5.1 INPUTS As you’ll recall from the previous chapter, inputs are the resources needed to implement the program. The most basic input of all is money, therefore one indicator is program cost. Impact evaluations produce estimates of the effectiveness of a program or intervention. Does the program “work”? For some public health and behavioral health nerds, evidence of impact is enough because they are narrowly focused on developing and testing new interventions. Not true for policymakers who are thinking about delivering programs at scale with limited public funding; they want to know whether the intervention is cost-effective, not just effective.39 A cost-effectiveness analysis requires close tracking of the cost of all program inputs. Patel et al. (2016) indicate that the HAP program costs $66 per person, or $181 per remission from depression at 3 months. 7.5.2 ACTIVITIES Treatment fidelity is a measure of how closely the actual implementation of a treatment or program reflects the intended design. The consequence of low treatment fidelity is usually an attenuation (aka, shrinking) of treatment effects. This is a threat to internal validity. If the study shows no effect but treatment fidelity is low, we can’t be confident in the null result. Implementation failure rather than theory or program failure could be to blame. Low fidelity is also a threat to external validity because it isn’t possible to truly replicate the study. Patel et al. (2016) measured fidelity in several ways, including external ratings of a random 10 percent of all intervention sessions. An expert not involved in the program listened to recorded sessions and compared session content against the HAP manual. 7.5.3 OUTPUTS Treatment compliance is a measure of the extent to which people (or units) were treated or not treated according to their study assignment. Sometimes people assigned to the treatment group don’t take up the treatment, or only complete part of the planned intervention. It’s also possible for members of the control or comparison group to be treated accidentally. Both are examples of broken randomization. When there is only non-compliance to randomization on the treatment side, we call it one-sided non-compliance. When some members of the control group are also non-compliant with randomization, it’s called two-sided non-compliance. Patel et al. (2016) randomly assigned 495 eligible adults to the HAP plus enhanced usual care condition (247) or the enhanced usual care condition alone (248). No one in the EUC only condition was treated with HAP, but 31 percent of the HAP group had an unplanned discharge and did not complete the treatment. We’ll discuss analysis strategies for one- and two-sided non-compliance in a later chapter. Additional Resources on Indicators Topic Resource Malaria Roll Back Malaria (2013). Household Survey Indicators for Malaria. Measure Evaluation (2016). Monitoring and Evaluation of Malaria Programs. HIV/AIDS WHO (2015). Consolidated Strategic Information Guidelines for HIV in the Health Sector. TB WHO (2015). A Guide to Monitoring and Evaluation for Collaborative TB/HIV Activities: 2015 Revision. Family Planning FP2020 (2015). Measurement Annex. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["datacollection.html", "8 Data Collection Methods 8.1 Quantitative Methods 8.2 Qualitative Methods 8.3 Mixed Methods Share Feedback", " 8 Data Collection Methods Now that you know what indicators you want to measure, it’s time to decide how to measure them. Sometimes it’s possible to use existing data sources, such as administrative or medical records, but most likely part or all of your data will come from original data collection efforts. In this chapter we’ll review common quantitative and qualitative methods that you can use. 8.1 Quantitative Methods An instrument is a tool for measuring indicators (Glennerster and Takavarasha 2013). Many studies in global health rely on survey instruments, so we’ll begin our tour with surveys before turning to non-survey instruments. 8.1.1 SURVEYS The most common type of data collection instrument in global health is surveys. Surveys are relatively cheap and easy to administer compared to some methods like biomarker testing, but care must be taken to pre-test the instrument, train enumerators, and monitor the administration. I’ll use the term ‘survey’ throughout this section, but some people would use the more specific term ‘questionnaire’ instead. This is because survey can represent the larger category of data collection that includes interviews. When interviews are structured or semi-structured, they look a lot like a written questionnaire that is read aloud to participants. Designing a survey instrument Start with standard tools Whenever possible, begin with well-known survey instruments. A great option for basic demographic and health questions is the DHS model questionnaires. There are four types plus several optional modules: household, woman, man, and biomarker. Household Woman Man Biomarker Optional Household schedule Background Background Anthropometry Domestic Violence Household characteristics Reproductive behavior and intentions Reproduction Anemia Female Genital Cutting Contraception Knowledge and use of contraception HIV Maternal Mortality Antenatal, delivery, and postnatal care Employment and gender roles Fistula Breastfeeding and nutrition HIV and other sexually transmitted infections Out-of-pocket Health Expenditures Children’s health Other health issues Status of women HIV and other sexually transmitted infections Husband’s background Other topics DHS questionnaires and analysis code are available for download, along with materials used in the AIDS Indicator Survey (AIS), Malaria Indicator Survey (MIS), Service Provision Assessment (SPA), and Key Indicators Survey (KIS). Writing good survey questions Sometimes you have to create your own surveys. Start early! Writing good survey questions is an art. It takes a lot of practice and trial &amp; error to get it right. Common problems include: Use of confusing or complex language Unclear meaning Use of double-negatives Embedding more than one question in a question (double-barreled) Use of leading statements Hard to answer The solution to all of these problems is pre-testing. Cognitive interviewing is a good technique for pre-testing in which you ask a question and record a response, but only as a way of inquiring about the respondent’s understanding of the question. For instance, let’s say the survey item is: Over the past 2 weeks, how often have you been bothered by feeling tired or having little energy? Not at all, several days, more than half the days, nearly every day. In cognitive interviewing, you would ask the respondent to explain the meaning or the purpose of the question. What do you think I am asking you to tell me? What does it mean to experience something for ‘more than half of the days’ in the past two weeks? If the respondent’s answers reflects a clear understanding of the question and response options, it’s probably a good item. If not, it might be worth exploring alternative phrasings that can be pre-tested with other members of your target population. These tests can take place one-on-one or in a group format. Some researchers advocate mixing positively and negatively worded questions to limit acquiescence bias, which happens when respondents get in the pattern of just agreeing (or disagreeing) when question after question follows a similar format. The potential downside of mixing directions is that respondents might have a harder time understanding the meaning of each question. Pilot-testing should limit this concern, however. Selecting response options Survey items are typically closed-ended rather than open-ended, meaning that the respondents are asked to provide a specific answer such as a number (e.g., age) or date, or are asked to pick from a set of possible options. When an item has more than one option, it is called a categorical variable. One type of categorical variable is a dichotomous variable (aka binary) that has two options, usually “yes” or “no”. When respondents pick from mutually-exclusive categories that don’t have any particular order, we call this a nominal variable (e.g., cow, pig, sheep). If the options are ordered, it’s called an ordinal variable (e.g., never, rarely, sometimes, often). One type of ordinal variable is a Likert-type item in which response options take a discrete value along a continuum with qualitative anchors. For instance, you might ask a respondent if they agree or disagree with a statement on a 4-point Likert-type scale: Strongly agree (0) Agree (1) Disagree (2) Strongly disagree (3) Without a “neutral” middle option, this response set would be referred to as forced choice because the respondent has to decide to be closer to ‘agree’ or ‘disagree’. This is often an advantageous survey design decision because you avoid having participants clump around this neutral middle point. Participants must be free to refuse to answer questions, but advertising the neutral or ‘don’t know’ option on questions about attitudes or beliefs tends to increase it’s use because it’s always easier not to make a decision.40 It’s also possible to position the two extremes of a scale on either end of a horizontal line and ask respondents to draw an intersecting vertical line somewhere along the line. This is called a visual analogue scale. Web or tablet administration makes it possible to ask the respondent to move a virtual slider to the desired position between the two anchors. Such scales blur the lines between ordinal and continuous measurement. Translating the survey It’s essential to prepare a high quality translation when materials are presented to participants in a different language. There are various approaches to translation, but one that seems to work well is forward translation by a skilled translator and then blind back-translation by a second skilled translator. The key word here is blind; the back translation is worthless if the translator has access to the original version. This process makes it possible to look for problems by comparing the original version and the back-translated version. Whenever a potential loss or change of meaning is detected, it’s necessary to review all three versions (original, translated, back-translated) and determine the cause. Once discrepancies are resolved, it is ideal to have translated instrument reviewed by language experts and subject matter experts. Avoid plans to translate ‘on-the-fly’. Know your participants and find the right translator. For instance, if you want to survey women in a rural area without much formal education, make sure that the translator does not introduce complex language and sentence structure that might be technically correct by utterly confusing for participants. Administering the survey There are two basic approaches to survey administration: Participants complete the survey on their own Trained enumerators read each question aloud and record answers Surveys conducted in low-income countries are often administered in person by trained enumerators who read each question aloud and record answers on paper41 or an electronic device (via computer-assisted personal interviewing, or CAPI). This is done primarily to ensure that illiterate people are not excluded. When survey need to ask about sensitive topics, researchers might employ audio or video computer-assisted self-interviewing tools (CASI) to allow the participant to complete the survey on their own. Format Administration Mode Data Capture Enumerator Present Label Paper Participant Read Participant Maybe Paper Enumerator Read Enumerator Yes Paper Enumerator Read Participant Yes Secret ballot Electronic Participant Read Participant Yes Computer-assisted personal interviewing (CAPI) Electronic Enumerator Read Enumerator Yes Computer-assisted personal interviewing (CAPI) Electronic Participant Read Participant No Computer-assisted self interviewing (CASI) Electronic Participant Listen Participant No Audio computer-assisted self interviewing (ACASI) Technology makes some things easier other things harder. On the positive side, electronic administration eliminates the time, cost, and errors associated with manual data entry. The ability to incorporate survey logic also prevents skip pattern errors. Some downsides include hardware costs and maintenance woes. The combination of rough use in field surveys and rapid developments in software and hardware mean that you might find yourself in a regular cycle of evaluating new options for collecting data. See here for more thoughts about how to answer the paper vs. digital question (which is increasingly becoming a question of “how to do digital data collection”). Online surveys are not commonly used in global health, but as Internet access grows platforms like Amazon’s Mechanical Turk are becoming more feasible for certain research questions. See here for a discussion of how researchers are using Mechanical Turk in mostly high-income settings to recruit study participants. Adapting instruments for new settings The BDI-II appeared to be an accurate measure of depression among HIV-positive adolescents in Malawi according to Kim et al. (2014). This is not always the case, however, when exporting scales developed and validated in one cultural context to another. Whenever you are considering using or adapting a survey instrument like the BDI-II for use in a new context, you should determine if the instrument is a valid measure of the construct in that new context. Translation alone does not make an instrument a valid tool for measuring a construct in a new socio-cultural setting. This advice from Kohrt et al. (2011) just about says it all: …instruments developed and validated with children in high income countries with Western cultural settings cannot simply be translated with the expectation they will have the same psychometric properties in other cultural contexts. Cutoff scores established with Western child populations are not necessarily comparable in other settings and may lead to misclassification and distortion of prevalence rates. Moreover, the instruments may not capture the constructs they are intended to measure in other cultural contexts where the meaning, clustering, and experience of symptoms often differs. You should be skeptical when you read that an instrument has been validated in a particular setting. Kohrt et al. (2011) give us six questions to appraise cross-cultural validity of instruments (with an emphasis on global mental health): Question Details 1. What is the purpose of the instrument? Just as an instrument developed in the U.S. might not be a valid measure of a construct in Japan, an instrument developed to assess prevalence of this construct might not be a valid measure of response to treatment. The validity of instruments should be evaluated based on purpose and context. As Kohrt et al. note: ‘Validity is not an inherent property of an instrument.’ 2. What is the construct to be measured? Construct validity refers to the extent to which an instrument measures the construct it is intended to measure. Construct validity is traditionally described as consisting of two parts: convergent and discriminant validity. Establishing construct validity in cross-cultural settings is more complex, however. In global mental health, we might examine three types of constructs through qualitative and ethnographic inquiry: local constructs (aka, idioms of distress or culture-bound syndromes), Western psychiatric constructs, and cross-cultural constructs. 3. What are the contents of the construct? Content validity is more specific and says that the components of an instrument—e.g., each question in a questionnaire—is relevant to the measurement of the larger construct. Content validity also accounts for missing dimensions of a construct. Is the instrument comprehensive? 4. What are the idioms used to identify psychological symptoms and behaviors? Language matters. The words we use to describe behaviors and inner states—idioms—can take on different meanings in different contexts. Semantic equivalence indicates that ‘the meaning of each item is the same in each culture after translation into the language and idiom (written or oral) of each culture’. 5. How should questions and responses be structured? As discussed above, it is hard to write good questions and structure response options that promote accurate measurement. Technical equivalence is demonstrated when, ‘the method of assessment… is comparable in each culture with respect to the data that it yields’. 6. What does a score on the instrument mean? When we talk about validating measures, we often mean establishing criterion validity: evidence that the instrument is an accurate measure of some outcome. Two subtypes of criterion validity are concurrent validity and predictive validity. Predictive validity assesses whether an instrument used today predicts some outcome measured in the future (e.g., standardized test scores and future performance in graduate school). Concurrent validity takes a present focus: Is the instrument a proxy for some gold standard, like a diagnosis of major depressive disorder by a mental health professional. This particular example might also be referred to as diagnostic validity. Of course it’s often impossible—or at least highly impractical—to establish a gold standard criterion in low-income settings where there are relatively few trained professionals, so researchers often try to establish that a new measure is correlated with existing validated measure, but there is a chicken and egg problem here. The bottom line is that it’s challenging to validate measures across cultures and typically involves a lot of detailed work. Essential work. As Kohrt et al. note: ‘The misapplication of instruments that have not undergone diagnostic validation to make prevalence claims is one of the most common errors in global mental health research.’ “So what if the instrument I want to use has not been validated?” It might be fine to use. Or it might not. The problem is that it can be impossible to tell the difference. If you don’t make an effort to prospectively answer these six questions and instead choose to just translate the instrument and move forward—or someone gives you a dataset to analyze from a completed study—then you might be out of luck. Sorry. Please keep this in mind: reliability and validity are established in particular samples. If your sample is representative of the population, reliability and validity should hold across new samples from this population. If you move to a new population, however, this becomes a very strong assumption. 8.1.2 NON-SURVEY INSTRUMENTS Surveys are widely used in global health, but researchers are constantly on the lookout for creative new low-cost and easy to use instruments that will obtain more valid measurements. Here are some examples that you will find in global health.42 Direct observation One alternative to participant recall and self-report is to observe behavior directly. Direct observation is often described as a qualitative method, but there are several approaches where the goal is primarily to quantify behavior. Random spot checks Random spot checks can be a good technique to use when participants might want to hide a certain behavior, such as not showing up for work. For instance, Chaudhury et al. (2006) studied teacher absenteeism in India by having study enumerators show up at schools unannounced to check whether the teacher was present, whether students were in class, and whether any classroom instruction was taking place. Mystery clients and incognito enumerators Mystery clients mask their identity and purpose to have an authentic experience and data collection opportunity. Kaur et al. (2015) used mystery clients in Nigeria to update a sampling frame of public and private facilities offering artemisinin-based combination therapies for the treatment of malaria. Mchome et al. (2015) took a more qualitative approach, training a dozen Tanzanian youth to visit health facilities and evaluate “youth-friendly” reproductive health services by requesting condoms and information on sexually transmitted infections and family planning. An incognito enumerator does something similar but does not participate as a client. Glennerster and Takavarasha (2013) give the example of going on a ‘ride along’ with drivers to count the number and amount of bribes they are compelled to pay along their route. Physical/environmental tests Physical tests are objective measures of materials and environmental substances. For instance, Rosa et al. (2014) conducted a trial of a combined intervention that involved distributing free water filters and improved cookstoves to hundreds of households in Rwanda. In addition to measuring use of these devices through self-report and spot-checks, the research team assessed (i) levels of fecal contamination (presence of thermotolerant coliforms in drinking water samples) and (ii) average 24-h concentrations of particulate matter in the main cooking area. Biological samples This is a large category of assessment that includes diagnostic tests and the analysis of specimens such as hair, saliva, toenail clippings, urine, and tissue biopsies, to name a few. These tests can determine the presence of disease (or markers of disease), predictors of disease, and consequences of disease (Jacobsen 2016). One of many examples comes from Obala et al. (2015) who used a rapid diagnostic test for malaria to confirm infection and caseness in a case-control study in western Kenya. Anthropometric measures Anthropometric measures quantify characteristics of the human body, such as height, weight, and mid-upper-arm circumference. Research on obesity and malnutrition features anthropometric measurement prominently, but you’ll find use cases in every area of global health. Use care to follow standard data collection techniques and methods of indicator construction (Cogill 2003,Blossner and Onis (2005)). Vital signs Vital signs include physiological measurements such as body temperature, blood pressure, pulse, and respiratory rate. Such measurements are typically easy to obtain. Some et al. (2016) demonstrated in Kenya that management of non-communicable diseases such as hypertension, diabetes mellitus type 2, epilepsy, asthma, and sickle cell, could be effectively shifted from clinical officers (roughly nurse practitioners in the U.S. healthcare system) to nurses to improve overall access to care by reducing the workload of more skilled providers. Clinical examination Medical personnel can also be trained to collect reliable data through a clinical examination. A clinician might listen to a patient’s heart, breath, and bowel sounds, or she might examine the patient’s eyes, ears, and hair. When used in a research context, examination procedures and data recording should be standardized, and data collectors should be trained until reliable. One example comes from Liu et al. (2016) who used the Structured Clinical Interview for Diagnostic and Statistical Manual for Mental Disorders (SCID) to assess depression among a sample of elderly participants in Hunan Province, China. The SCID is structured in the sense that the examination has detailed steps and includes an algorithm for making a clinical determination about depression. This standardization makes it easy to train data collection personnel and reduce measurement error. Tests of physiological function Some studies use tests that measure physiological function, such as electrocardiography (ECG) to measure heart function and electroencephalography (EEG) to measure brain function. Lelijveld et al. (2016) measured lung function with spirometry in a cohort of Malawian children treated for severe acute malnutrition. Medical imaging Medical imaging techniques create visual representations of internal body structures. Examples include radiography (X-rays), computed tomography (CT) scans, magnetic resonance imaging (MRI), and ultrasound (Jacobsen 2016). For instance, Rijken et al. (2012) studied the effects of malaria infections early in pregnancy on fetal growth by using ultrasound to measure fetal biparietal diameter and comparing outcomes among infected and uninfected women. Tracking devices This category of instruments is growing rapidly. From radio-frequency identification (RFID) to track attendance to wearable digital health technology to monitor activity, researchers have a large selection of consumer and professional tracking devices for measuring individual behavior. Vanhems et al (2013) used wearable proximity sensors (RFID) to measure contacts (frequency and duration) among patients and healthcare workers in a geriatric unit of a hospital in France. GIS and remote sensing In recent years, researchers in global health have found many applications for geographic information systems (GIS), global positioning systems (GPS), and remote sensing (e.g., satellite imagery). A particularly active area of research has been the spatial epidemiology of malaria. For instance, Sewe et al. (2016) used satellite imagery to measure the Normalized Difference Vegetation Index, day Land Surface Temperature, and precipitation, and examined the relationship between these environmental variables and malaria mortality. Standardized tests As a measure of knowledge, standardized tests are useful instruments to estimate the immediate impact of a program or intervention designed to teach people new ideas or skills. For example, HIV prevention programs often include some measure of HIV knowledge (Hughes and Admiraal 2011). As many studies have demonstrated, however, increasing knowledge does not always translate to changing behavior. Vignettes Another method of assessing knowledge (and attitudes and skills) is to present participants with hypothetical scenarios called vignettes. Corneli et al. (2015) used vignettes to study women’s differential likelihood of engaging in risky sexual behavior if taking pre-exposure prophylaxis (PrEP) for HIV. Discrete choice experiments (DCE) are a particular type of vignette commonly used in healthcare policy and economics. In a DCE, participants are presented with the same basic vignette that combines variations in attributes to elicit the factors that have the greatest influence on preferences [mangham:2009]. For instance, Michaels-Igbokwe et al. (2015) designed a DCE with variations on six attributes to measure youth’s preferences for family planning service providers in Malawi. The six attributes included distance to the provider, hours of operation, waiting time, provider attitude, stock, and price. Given the large number of combinations possible, the authors used a fractional factorial design to present participants with a limited choice set. Provider attitude and stock appeared to be important drivers of uptake. Behavioral games A related idea is to design “games” in which participants are asked to make decisions with real money to measure constructs like time preferences, trust, risk aversion, and prosocial behavior. Blattman et al. (2016) used games in a randomized trial of a microenterprise development program in Uganda to measure how future orientation moderated program impacts. List randomization It is challenging to obtain accurate and honest answers to sensitive questions, such as questions about sexual behavior. One approach to promoting more honest responding is the unmatched count technique, aka list randomization. In this technique, researchers create two lists of questions or statements. The lists are identical, except that one list has an additional question to measure the sensitive issue. Participants are randomly assigned to get the shorter or longer list. Everyone is asked to indicate the total number of correct or truthful items rather than providing an answer to each item. The difference in the mean number of endorsed items per list can be interpreted as the proportion of participants who endorsed the sensitive issue. For instance, if one list has 5 items and the other list has 6 items, mean counts of 3.4 and 3.9, respectively, would indicate that 50 percent of participants endorsed the sensitive item. Karlan and Zinman (2012) used this technique in Peru and the Philippines to measure the proportion of loan recipients who used loan proceeds for non-enterprise purchases. A limitation of this approach is that estimates are only available at the level of randomization. Purchasing decisions Another alternative to self-report is measuring purchasing decisions. For instance, Dupas (2014) returned to a sample of Kenyan households who were randomly assigned to receive a new type of bednet at prices that ranged from $0 to $3.80. When the research team returned a year later, all households were offered an opportunity to purchase another bednet at the subsidized price of $2.30. They examined how partial or full subsidies in the first phase affected a household’s willingness to purchase a bednet in the second phase. By measuring purchasing decisions, the researchers were able to avoid relying on household self-report about whether they would purchase bednets. Social networks Social relationships influence the spread of disease and offer opportunities for health interventions. Researchers measure these relationships as social networks. Kelly et al. (2014) review examples of how to collect and analyze social network data in low-income settings. 8.2 Qualitative Methods Whereas a good survey requires uniformity and structure, a good interview is flexible and probing. This flexibility is what makes qualitative methods well-suited for exploratory and descriptive research. Flexibility to examine questions of how and why. The three most common qualitative methods are in-depth interviews, focus groups, and participant observation (Mack et al. 2005). Dr. Leslie Curry at the Yale School of Public Health produced six video modules on qualitative research that readers might find useful. You can watch all six videos here. 8.2.1 IN-DEPTH INTERVIEWS An in-depth interview is a method of eliciting a person’s views and stories. The interviewee is the expert, and it’s the interviewer’s role to learn from this expertise. Typically interviews are conducted as a private conversation between two people. Some interviews are highly structured with a pre-determined set of questions and follow-up probes. Other interviews are designed to explore a general research topic and do not follow a specific format. All interviews need to be managed, however. The interviewer must guide the participant enough to find the desired narrative within the time constraints. Data generated by interviews can include audio and/or video recordings that become transcripts, interviewer field notes, and analysis memos. Data quality typically hinges on the skills of the interviewer. To obtain rich, thick description of events and perspectives, the interviewer must keep the participant talking. This takes a substantial amount of practice to do well. Mack et al. (Mack et al. 2005) point to three key skills that interviewers should master: Rapport-building: An interviewer must quickly make the participant feel at ease and free to talk openly and honestly. Emphasizing the participant’s perspective: This requires an interviewer to mask their own perspectives and treat the participant as the expert. People talk when they believe that the person across from them is truly listening and engaging in what they are saying. Adapting to different personalities and emotional states: Every interview is different. The interviewer must be able to adapt his or her interview style to match the needs and style of the participant. Interviewers must also learn to ask clear, open-ended questions and probe responses effectively. Common interviewing mistakes include asking multiple questions at once and asking closed-ended questions that signal the need for a yes/no or short answer. New interviewers also make the mistake of moving on to the next question too quickly, or failing to listen to the participant’s response because they are thinking of the next question to ask. There are two main strategies for eliciting more information from participants: asking follow-up questions and probing. Sometimes interview guides will specify sub-questions that should be explored if a participant’s initial response does not address all of the important issues. Other times follow-up questions are ad hoc and intended to clarify a participant’s statement or pursue an interesting idea. Specifying follow-up questions in advance can be a useful technique, but it can also create a tendency for new interviewers to move into “survey mode” where the goal is to get some answer for every question, rather than to explore the topic in a more open-ended fashion. Often, the better approach to keep people talking is to use probes. Probes can be direct questions or indirect expressions or visual cues for a participant to say more. Effective direct probes include: Can you say more about that? I’m not sure I understand. Can you explain? Can you give me an example? Why do you think…? Examples of indirect probes include: Verbal expressions that indicate you are an active listener, such as “uh huh”, “ok”, and “I see”. Restating the participant’s perspective, e.g., “So you believe that bednets are really only useful during the rainy season”. Reflecting the participant’s feelings, e.g., “And that made you feel disrespected”. Non-verbal gestures to signal that you are listening, such as nodding. Combining direct and indirect probes with clarifying questions is typically a winning combination, but it takes a substantial amount of practice to do well. And it’s only one interviewing skill to master. The other main skill is “managing” the interview. This means watching the clock and deciding how to balance depth and breadth of responding, redirecting participants when the conversation gets too far afield, and taking good notes. Doing this while sustaining rapport, monitoring a participant’s experience, and maintaining an awareness of what ground the interview has and has not covered is incredibly difficult. It is relatively easy to teach someone to be a good survey enumerator—to ask specific questions and record discrete answers. It is much harder to teach someone to be a good interviewer. Whereas you might be able to run a survey training over the course of a few days or a week, training qualitative interviewers could easily take a month or more to allow enough cycles of practice and feedback. 8.2.2 FOCUS GROUPS While interviews are a good method for understanding individual perspectives, focus groups are a better choice if the goal is to quickly gain insight into group norms and the range in group opinions. Focus groups can explore individual experiences to learn about these issues, but private stories are best left for individual interviews where confidentiality and privacy are under the interviewer’s control. It’s often most effective for two people to tag team a focus group with one playing the role of moderator and the other taking notes and preparing materials. The moderator has a difficult role to play. Just as an interviewer has to manage the interview process, the moderator has to manage the focus group. This sometimes means redirecting the conversation away from certain topics and dominant participants to achieve certain goals within a set time period. This job becomes harder as groups grow in size as it can be a challenge to involve everyone and make sense of too many voices. There is no cap on how many participants can join a focus group, but more than a dozen would likely be too hard to manage. It can be helpful to begin a session by laying some ground rules for the discussion that encourage respect for all participants. This gives the moderator something to refer back to when someone in the group is having a negative impact on the discussion. Once the rules are established, it’s the moderator’s responsibility to ensure that these rules are followed. Focus group discussions are usually most interesting when the moderator is able to encourage productive crosstalk between participants. Typically discussions begin as a series of 2-way exchanges between the moderator and a participant. To break out of this pattern and encourage participants to respond to each other, the moderator can follow a participant’s contribution with a question like “Who takes a different view?” or “What do you think of this idea?” and use non-verbal cues to signal that other people should weigh in. If you have permission to capture an audio recording of the session, it is helpful to identify participants when they speak. For instance, the notetaker might create a basic matrix of participant demographics (e.g., age, gender, education) and give each participant an identification number. By placing cards with ID numbers in front of each participant during the discussion, the moderator is able to say something like “Go ahead #3” to link voices to demographics in the audio recording. This pattern can be cumbersome to maintain, so a notetaker should also capture ID numbers along with viewpoints. Focus group activities do not need to be limited to discussion of questions posed by the moderator. Sometimes it can be helpful to design short exercises to stimulate conversation. For instance, free listing is a technique where participants brainstorm on a particular issue and the notetaker records ideas in rapid succession. The moderator might ask the group to think of what “depression” looks like among pregnant women and new mothers in that particular community. A potential follow-up activity is card sorting in which the group ranks the free listing ideas and/or sorts them into conceptual piles. As each activity unfolds, the moderator finds opportunities to probe and engage participants in a discussion. 8.2.3 PARTICIPANT OBSERVATION Another approach is participant observation—making observations while participating to some extent in community life. This is not the same as some of the ‘observation-for-quantification’ approaches we discussed earlier, such as random spot checks. Participant observation typically happens over a much longer time—weeks, months, or years—and is designed to result in ‘thick description’ of context, attitudes, and behaviors. Sometimes participant observation is used as a formative research step to inform the development of interview and focus group guides or to generate hypotheses. This method is also used after quantitative data collection to understand more about why an intervention might not have worked, or to triangulate quantitative findings. If your study aims are more ethnographic in nature, you might find yourself engaging in participant observation as a primary method of data collection. Mack et al. (2005) remind us that observers always remain “outsiders” to some extent, thus it is important to document observations without the filter of interpretation, which can often be wrong. The authors give the example of observing two men holding hands in a place like Uganda—where men will often hold hands as a sign of friendship—and making the incorrect conclusion that the men are homosexual. Interpretation and questioning of one’s observations is better documented in research memos (analysis) rather than field notes. If you are training team members to become participant observers, it is important to discuss strategies for how they will blend in with the community. This involves thinking about things like dress, mannerisms, and behavior. Attempting to participate in community life can lead to an authentic experience, but sometimes full participation is not wise. For instance, there are lines that should not be crossed when it comes to illegal behavior or sexual relationships with participants. Having regular supervision or mentoring meetings with study staff can help make sense of grey areas. It’s common to find references to “Grounded Theory” in qualitative work. Grounded theory is an iterative methodology for collecting and analyzing data. Data collection is iterative in the sense that the focus can shift over time as you reach saturation, the point at which you begin hearing the same themes over and over. Analysis involves coding data (e.g., text, photos, videos, observations, etc) based on emergent themes. Sampling procedures in grounded theory tend to follow gaps in the data, rather than a random process. As you collect and analyze data, you write research memos that themselves become sources of data. Hypotheses inductively emerge from this process of collection and analysis, and some grounded theorists attempt to close the loop by testing hypotheses with additional data collection and analysis. 8.3 Mixed Methods In mixed methods research, qualitative and quantitative methods are like peanut butter and jelly. Better together. Mixed methods guru John Creswell defines mixed methods research as: An approach in the social, behavioral, and health sciences in which the investigator gathers both quantitative (closed-ended) and qualitative (open-ended) data, integrates the two, and then draws interpretations based on the combined strengths of both sets of data to understand research problems. In this formulation, mixing is the key characteristic of mixed methods research. A study that uses qualitative and quantitative methods to collect data but never brings the data together is probably not a mixed methods study. A mixed methods research design is often a good choice when: qualitative or quantitative methods alone are insufficient to fully answer the research question you need to develop quantitative tools through exploratory research you need to triangulate results you need to better understand results Creswell outlines three basic mixed methods designs: convergent, explanatory sequential, exploratory sequential. Label Details Convergent qualitative and quantitative data are collected separately and then merged in the analysis phase so that the results reflect joint interpretation Explanatory sequential begin with quantitative work and then later turn to qualitative inquiry to help explain the quantitative results Exploratory sequential begin with qualitative work to explore an issue, create new instruments, or develop new interventions, and then later apply this learning in a quantitative study of the issue A nice example comes from Bass et al. (2008), a mixed methods study of post-partum depression in the Democratic Republic of Congo. The authors conducted a round of qualitative research to develop a pool of screening items and then conducted a quantitative validation study to evaluate the criterion validity of the new instrument. Share Feedback This book is a work in progress. You’d be doing me a big favor by taking a moment to tell me what you think about this chapter. References "],
["glossary.html", "9 Glossary", " 9 Glossary Term Definition applied research research focused on specific problems or applications basic research pursuit of fundamental knowledge of phenomena; forms the basis of clinical research behavioral research Research that focuses on the most effective ways to change people’s behaviors in their daily life causal inference determining whether or not there is a relationship where one variable causes another; seeks to establish that X causes Y clinical research a broad field that encompasses patient-oriented research, epidemiological and behavioral studies, and outcomes research and health services research correlational research A type of descriptive research that asks questions about the relationship (a.k.a association) between two or more variables; builds upon descriptive insights by attempting to predict or explain the behavior or phenomenon correlational research cross-sectional deductive reasoning reasoning that makes conclusions about some unobserved or unmeasured phenomenon based on direct observations of the world demographic research Research that seeks to understand more about population size, structure and change (e.g., birth, death migration, marriage, employment, education) dependent variable Also called the response variable or descriptive inference goes beyond basic description (or collection of facts) to say something about how indiviudal experiences and opinions illuminate something more universal about the research problem at hand descriptive research Research that seeks to answer the question “What is going on?” empirical evidence the systematic observations that are used in the process of empiricism empiricism to use what observations to make conclusions about what cannot or will not be observed directly; at the heart of scientific research experimental design a design where researchers manipulate some independent variable and examine changes to some dependent variable that result (considered the “gold standard” of designs) explanatory research Research that seeks to answer the question “Why is it going on?” implementation failure The failure of a program to be effective because the program was not executed properly implementation science Studies that assess to how to best get efficacious treatments to the people who need it most inductive reasoning reasoning from specific observations to the generation of hypotheses and theories inference the process of making conclusions about some unobserved or unmeasured phenomenon based on our direct observations of the world longitudinal mixed methods designs that utilize both inductive and deductive reasoning needs assessments panel peer review an important compnent of the scientific process where a scientist’s peers evaluate a work before it is published in a journal Phase I In clinical research, a trial that utilizes a small sample size in order to find a safe dosing range and look for side effects. Phase II In clinical research, studies that demonstrate the efficacy of the medication against several endpoints (a.k.a. outcomes). Includes Phase IIa and Phase IIb. Phase III In clinical research, typically a large trial to show that a treatment is efficacious. Phase IV In clinical research, trials that evaluate a medication’s long-term effects. preclinical research The clinical research phase in which testing is performed in non-human subjects with the goal of collecting data on how well the medication works (efficacy), how much damage it can do to an organism (toxicity), and how it is affected by the body (pharmacokinetics). prevalence Prevalence is the number of existing cases out of the total population at a point in time. In contrast to incidence, which is a measure of disease occurrence, prevalence is a measure of existing disease. Prevalence is unit-less. program evaluation program monitoring Concerned with the implementation of programs, policies or interventions. Necessary for good evaluations. quasi-experimental design replication refers to the ability of another research group to be able to follow the methods of a research study and replicate the results; relatively rare representative reproducibility the ability to generate a study’s findings given the orginal dataset and sometimes the original analysis code research problem A gap in the academic world’s knowledge research question sample In statistics, a sample refers to a set of observations drawn from a population. scientific research Resarch where 1) the goal is inference, 2) the procedures are public, and 3) the conclusions are uncertain stakeholders Refers to a wide range of people and organizations that have some sort of vested interest in the outcome of a program’s implementation theory failure The failure of a program to be effective because the idea or theory behind the program was incorrect translational research Studies that focus on getting interventions from “bench to bedside” "],
["references.html", "10 References", " 10 References "]
]
